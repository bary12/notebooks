{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Q2AD' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Q2AD/Q2AD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Q2AD/Q2AD_v1_QuestionsID.json', 'r') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "with open('Q2AD/Q2AD_v1_train.json', 'r') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "with open('Q2AD/Q2AD_v1_test.json', 'r') as f:\n",
    "    test = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "    {\n",
    "        'question': questions[question_id],\n",
    "        'answer': item['full_text'],\n",
    "    }\n",
    "    for question_id, item in train.items()\n",
    "]\n",
    "\n",
    "test = [\n",
    "    {\n",
    "        'question': questions[question_id],\n",
    "        'answer': item['full_text'],\n",
    "    }\n",
    "    for question_id, item in test.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/ubuntu/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/vocab.json\n",
      "loading file merges.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 3,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": [\n",
      "    2\n",
      "  ],\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 62,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 11,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 6,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"save_step\": 58,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {},\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/config.json\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 3,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": [\n",
      "    2\n",
      "  ],\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 62,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 11,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 6,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"save_step\": 58,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {},\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-xsum-12-3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-xsum-12-3')\n",
    "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-xsum-12-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=2,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,  # demo\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-04,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        tokenized = tokenizer(item['answer'], return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        tokenized_question = tokenizer(item['question'], return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        labels = tokenized_question['input_ids'][0]\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        new_data.append({\n",
    "            'input_ids': tokenized['input_ids'][0],\n",
    "            'attention_mask': tokenized['attention_mask'][0],\n",
    "            'labels': labels\n",
    "        })\n",
    "    return new_data\n",
    "\n",
    "train_preprocess = preprocess(train)\n",
    "test_preprocess = preprocess(test)\n",
    "\n",
    "train_loader = list(DataLoader(train_preprocess, batch_size=training_args.per_device_train_batch_size, shuffle=True))\n",
    "test_loader = list(DataLoader(test_preprocess, batch_size=training_args.per_device_eval_batch_size, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_preprocess,\n",
    "    eval_dataset=test_preprocess,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 240\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 120\n",
      "  Number of trainable parameters = 255120384\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 02:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.382500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=4.760571734110514, metrics={'train_runtime': 121.5242, 'train_samples_per_second': 3.95, 'train_steps_per_second': 0.987, 'total_flos': 297195798528000.0, 'train_loss': 4.760571734110514, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What sport should I try out?\n",
      "Prediction:  What is one of the most popular sports in the world?\n",
      "Answer:  Table tennis is tremendously fun, and a much better source of exercise than people tend to give it credit for. It’s simultaneously a very intense mental sport and one that engages a lot of different muscle groups. As an added plus, people who don’t play it will think you’re amazing if you can play the game at anywhere above a novice level!\n",
      "--------------------------------\n",
      "Question:  What is the point of a tattoo?\n",
      "Prediction:  What would you do if you got a tattoo?\n",
      "Answer:  There are a variety of reasons for getting a tattoo. You can get one to remember someone/something, you can get one because it looks pretty or cool, or a variety of other reasons. Reasons will certainly differ from person to person. I got my tattoo to remember my grandmother, the depression I experienced after her death, and to celebrate how my partner helped me get through that dark time.\n",
      "--------------------------------\n",
      "Question:  If we don&#039;t take revenge, how will they learn from their mistakes?\n",
      "Prediction:  How do you forgive someone who hurt them?\n",
      "Answer:  Forgive and show kindness. They will realize it themselves someday. Forgiveness is your trait. It solely depends on you and not on the other person. You want to forgive them because you want peace of mind and don't want to hold grudges against anyone in your life. Forgive because you have a strong heart. Forgive because you are strong. Strongest people are those who forgive people who aren’t even sorry for hurting you. Forgive but don’t forget the lessons learnt.\n",
      "--------------------------------\n",
      "Question:  Do people driving Corvettes get more attention from police?\n",
      "Prediction:  Should I stop a car from driving a $80,000 car?\n",
      "Answer:  Not really. I have stopped literally thousands of cars in my career, all makes and models. We don't look for a particular model, we look and observe driving behavior. Generally, a person driving an $80,000 Corvette isn't screwing around and usually doesn't do something that would cause them to become involved in a collision. Yet, on occasion, there is an exception to every rule.\n",
      "--------------------------------\n",
      "Question:  What&#039;s so scary about &quot;being cheated on&quot;?\n",
      "Prediction:  Why is it so hard to deal with a cheat in the past?\n",
      "Answer:  Because no one wants to feel betrayed and their trust being broken. No one wants to feel stupid for being trusting enough. The most heart shattering part of cheating, which actually hurts, is “lying”. When someone lies to you, you feel betrayed and that is hard to overcome.“Lying is about controlling someone else’s reality, hoping that what they don’t know won’t hurt you.”Neil Strauss, The Truth: An Uncomfortable Book about RelationshipsIf your partner betrays your trust by cheating (emotionally or physically or both), that is hard to deal with because you often feel stupid, stupid enough to trust your partner in the first place. And tell you what, no one wants to feel that they are stupid in the first place.\n",
      "--------------------------------\n",
      "Question:  What will happen if woman takes Viagra?\n",
      "Prediction:  Why is Viagra not a blood pressure medication?\n",
      "Answer:  It increases her blood pressure. Viagra was originally designed to be a medication for low blood pressure. It had a peculiar side effect for men though, and it was discarded as a blood pressure medication because it was much more profitable as an erectile disfunction medication. At the time there wasn't much available for ED while there were already several blood pressure medications out there.\n",
      "--------------------------------\n",
      "Question:  Is it conceivable, that Melania Trump is in fact a Russian agent, or can that be ruled out as just another crazy conspiracy theory?\n",
      "Prediction:  Is the wife of US President Donald Trump a spy?\n",
      "Answer:  It is conceivable. There is someting unexplainable about their relationship. She isn’t living with him. Her body laungage demonstrates tha she does not want to be near him. She sabotaged him by plagerizing the speech of the wife of the previous president causing the media to focus on her speech rather than his. HOWEVER , if she is a spy her handler would not allow this behavior.\n",
      "--------------------------------\n",
      "Question:  Which fruit to eat in empty stomach?\n",
      "Prediction:  What is the best fruit in the morning?\n",
      "Answer:  If you are so particular to eat a fruit in the morning, banana is the best fruit. Eat it or avoid it. While taking fruits in empty stomach takes fruits alone. Have your breakfast one hour after taking a Banana. Banana contains the three sugars-Glucose, Fructose (fruit sugar) and Galactose), besides useful fibers. It gives you an instant boost of energy for your morning workout. In addition the high amount of Potassium protects you from anemia. It is a good medicine for hypertension also.\n",
      "--------------------------------\n",
      "Question:  My mom just bought me &quot;The Metaphysics of Evolution&quot; by Father Chad Ripperger. Is this a trustworthy book?\n",
      "Prediction:  How do I read a book on the issue of evolution?\n",
      "Answer:  As many others have said, the book seems problematic, though I expect it does explain the beliefs of its author. The only reviews I could find were the few positive ones on The Metaphysics of Evolution: Evolutionary Theory in Light of First Principles - Kindle edition by Fr. Chad Ripperger. Religion & Spirituality Kindle eBooks @ Amazon.com. The author isn’t exploring the issue from a scientific point of view but instead approaching it in terms of his vision of Catholicism, one which disagrees with the official church stance. When you and your mom discuss the books, perhaps this can be one of your points of discussion—how one should evaluate ideas, particularly when religion and science take such different approaches. I think it’s great that you’ve agreed to read each others’ books and think this will be a good way for you to start an ongoing dialogue about the topic. If you keep asking each other questions you should be able to come to understand one another. You may also want to read some books by Kenneth R. Miller. He is both a known biologist and a practicing Catholic. This might give you some insights into those who understand evolution while also maintaining their religious beliefs. You can also find many lectures by Miller on YouTube.\n",
      "--------------------------------\n",
      "Question:  How many states are there in Pakistan?\n",
      "Prediction:  What are some of the countries that are divided into?\n",
      "Answer:  Unlike India and most other countries,Pakistan is actually divided into 4 provinces,name Khyber Pakhtunkhva Region,Punjab,Sindh and Balochistan. These are each further divided into many districtsAs of 2015,there are a total of 106 districts,divided all ost equally. Hope you get it.\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    encoding = tokenizer(test[i]['answer'], return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "    input_ids = encoding['input_ids'].to('cuda')\n",
    "    attention_mask = encoding['attention_mask'].to('cuda')\n",
    "    result = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=4, early_stopping=True)\n",
    "    prediction = tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "    print('Question: ', test[i]['question'])\n",
    "    print('Prediction: ', prediction)\n",
    "    print('Answer: ', test[i]['answer'])\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
