{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.batch_encode_plus(['Barack Obama was the 44th president of the United [MASK].'], return_tensors='pt')\n",
    "output = model(\n",
    "    input_ids=tokenized['input_ids'],\n",
    "    attention_mask=tokenized['attention_mask'],\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.hidden_states[0][:,-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "band film set guard hard late age time front term\n",
      "exchange guard union council president flag time section group part\n",
      "group exchange ) guard council division opera region organization year\n",
      "union organization exchange council group government year republic foundation division\n",
      "organization states federation republic nation union division sector organisation state\n",
      "states nation kingdom union organization federation nations republic division corporation\n",
      "states kingdom nation nations ireland republic federation union division century\n",
      "states kingdom nations nation ireland walesois provinces republicos\n",
      "states kingdom nations wales nation ireland union provinces netherlands republic\n",
      "states kingdom nations nation republiclts georgia netherlands us philippines\n",
      "states kingdom nations us nation state republic democrats americans front\n",
      "states nations kingdom nation state us president barack republic obama\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for i, layer in enumerate(output.hidden_states):\n",
    "    if i > 0:\n",
    "        # diff = torch.cosine_similarity(layer[:,-1,:], output.hidden_states[i - 1][:,-1,:], dim=1).item()\n",
    "        out = model.cls(layer[:,-3,:])[0]\n",
    "        out = torch.topk(out, 10).indices\n",
    "        print(tokenizer.decode(out))\n",
    "        # print(tokenizer.decode(out))\n",
    "        # diff = torch.cosine_similarity(layer[:,-1,:], model.bert.embeddings.word_embeddings.weight, dim=1)\n",
    "        # print(diff.shape)\n",
    "        # diff = tokenizer.decode(torch.topk(diff, 10).indices, skip_special_tokens=True)\n",
    "        # diff = torch.norm(layer[:,-1,:], dim=1).item()\n",
    "        # print(i, diff)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
