{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 17:18:49.110812: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 17:18:49.467757: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-21 17:18:49.467921: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-21 17:18:51.568982: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 17:18:51.569482: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 17:18:51.569535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bart.modeling_bart import BartForConditionalGeneration, BartEncoder, BartConfig, BartDecoder, shift_tokens_right\n",
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3a0446e9244cfa9908ecae99066bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b550fe1008044da8ed04e73c03714b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5904f8f70ade41cb829ed15fe278c336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a140cdfe73a4bd885b2879df51e7ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515e24194eed4bd7a8ba4be6d20bd5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/460M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\t\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-xsum-6-6')\n",
    "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-xsum-6-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bary/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 62 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penguins have been released into the wild for the first time.\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer('Penguins, unlike most birds, they can\\'t', return_tensors='pt')\n",
    "output = model.generate(**tokenized)\n",
    "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, List, Tuple, Union\n",
    "from transformers.models.bart.modeling_bart import BartEncoderLayer, BartDecoderLayer, BartConfig\n",
    "\n",
    "\n",
    "class MyDecoderLayer(nn.Module):\n",
    "    def __init__(self, layer: BartDecoderLayer, config: BartConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = layer.embed_dim\n",
    "        self.self_attn = layer.self_attn\n",
    "        self.dropout = layer.dropout\n",
    "\n",
    "        self.activation_fn = layer.activation_fn\n",
    "        self.activation_dropout = layer.activation_dropout\n",
    "\n",
    "        self.self_attn_layer_norm = layer.self_attn_layer_norm\n",
    "        self.encoder_attn = layer.encoder_attn\n",
    "        self.num_encoder_attns = 2\n",
    "        self.encoder_attns = nn.ModuleList([copy.deepcopy(layer.encoder_attn) for _ in range(self.num_encoder_attns)])\n",
    "        self.encoder_attn_layer_norm = layer.encoder_attn_layer_norm\n",
    "        self.fc1 = layer.fc1\n",
    "        self.fc2 = layer.fc2\n",
    "        self.final_layer_norm = layer.final_layer_norm\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = True,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            encoder_hidden_states (`torch.FloatTensor`):\n",
    "                cross attention input to the layer of shape `(num_context_sequences, batch, seq_len, embed_dim)`\n",
    "            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n",
    "                size `(decoder_attention_heads,)`.\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Self Attention\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Cross-Attention Block\n",
    "        cross_attn_present_key_value = None\n",
    "        cross_attn_weights = None\n",
    "        if encoder_hidden_states is not None:\n",
    "            residual = hidden_states\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            hidden_states_all = []\n",
    "            cross_attn_weights_all = []\n",
    "            cross_attn_present_key_value = []\n",
    "            for i in range(self.num_encoder_attns):\n",
    "                hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
    "                    hidden_states=hidden_states,\n",
    "                    key_value_states=encoder_hidden_states[i,:,:,:].squeeze(0),\n",
    "                    attention_mask=encoder_attention_mask,\n",
    "                    layer_head_mask=cross_attn_layer_head_mask,\n",
    "                    past_key_value=cross_attn_past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "                hidden_states_all.append(hidden_states)\n",
    "                cross_attn_weights_all.append(cross_attn_weights)\n",
    "                cross_attn_present_key_value.append(cross_attn_present_key_value)\n",
    "\n",
    "            hidden_states_all = torch.stack(hidden_states_all, dim=0)\n",
    "            cross_attn_weights_all = torch.stack(cross_attn_weights_all, dim=0)\n",
    "            cross_attn_present_key_value = torch.stack(cross_attn_present_key_value, dim=0)\n",
    "\n",
    "            hidden_states = hidden_states_all.mean(dim=0)\n",
    "            cross_attn_weights = cross_attn_weights_all.mean(dim=0)\n",
    "            cross_attn_present_key_value = cross_attn_present_key_value.mean(dim=0)\n",
    "\n",
    "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "            hidden_states = residual + hidden_states\n",
    "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights, cross_attn_weights)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(model.model.decoder.layers)):\n",
    "    layer = model.model.decoder.layers[i]\n",
    "    new_layer = MyDecoderLayer(layer, model.model.config)\n",
    "    model.model.decoder.layers[i] = new_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = model.model.encoder(**tokenized, output_hidden_states=True, output_attentions=True)\n",
    "output2 = model.model.encoder(**tokenized, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states = torch.stack((output1.last_hidden_state, output2.last_hidden_state))\n",
    "decoder_input_ids = tokenizer.encode('', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.model.decoder(encoder_hidden_states=encoder_hidden_states, input_ids=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31.3633,  2.8388,  1.5540,  ...,  3.7043,  3.5683, -1.6295],\n",
       "        [ 1.6151, -1.2754,  4.4009,  ..., -1.5303, -1.5515, -2.1655]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head(output.last_hidden_state) + model.final_logits_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecoder_head_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcross_attn_head_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpast_key_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecoder_inputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeq2SeqLMOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "The [`BartForConditionalGeneration`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "the latter silently ignores them.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Args:\n",
      "    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "        Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
      "        it.\n",
      "\n",
      "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "        [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "        [What are input IDs?](../glossary#input-ids)\n",
      "    attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 for tokens that are **not masked**,\n",
      "        - 0 for tokens that are **masked**.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
      "        Indices of decoder input sequence tokens in the vocabulary.\n",
      "\n",
      "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "        [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "        [What are decoder input IDs?](../glossary#decoder-input-ids)\n",
      "\n",
      "        Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n",
      "        is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n",
      "\n",
      "        For translation and summarization training, `decoder_input_ids` should be provided. If no\n",
      "        `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n",
      "        for denoising pre-training following the paper.\n",
      "    decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
      "        Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n",
      "        be used by default.\n",
      "\n",
      "        If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n",
      "        and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
      "        information on the default strategy.\n",
      "    head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
      "        Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 indicates the head is **not masked**,\n",
      "        - 0 indicates the head is **masked**.\n",
      "\n",
      "    decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
      "        Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 indicates the head is **not masked**,\n",
      "        - 0 indicates the head is **masked**.\n",
      "\n",
      "    cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
      "        Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n",
      "        1]`:\n",
      "\n",
      "        - 1 indicates the head is **not masked**,\n",
      "        - 0 indicates the head is **masked**.\n",
      "\n",
      "    encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
      "        Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n",
      "        `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n",
      "        hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
      "    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "        `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
      "\n",
      "        Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "        blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
      "\n",
      "        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "        `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape\n",
      "        `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you\n",
      "        can choose to directly pass an embedded representation. This is useful if you want more control over how to\n",
      "        convert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
      "    decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n",
      "        Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n",
      "        representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n",
      "        input (see `past_key_values`). This is useful if you want more control over how to convert\n",
      "        `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
      "\n",
      "        If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n",
      "        of `inputs_embeds`.\n",
      "    use_cache (`bool`, *optional*):\n",
      "        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "        `past_key_values`).\n",
      "    output_attentions (`bool`, *optional*):\n",
      "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "        tensors for more detail.\n",
      "    output_hidden_states (`bool`, *optional*):\n",
      "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "        more detail.\n",
      "    return_dict (`bool`, *optional*):\n",
      "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      "        config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      "        (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      "\n",
      "\n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.Seq2SeqLMOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.Seq2SeqLMOutput`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`BartConfig`]) and inputs.\n",
      "\n",
      "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Language modeling loss.\n",
      "        - **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) -- Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
      "        - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "          `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
      "\n",
      "          Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "          blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
      "        - **decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "          Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
      "        - **decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
      "          self-attention heads.\n",
      "        - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
      "          weighted average in the cross-attention heads.\n",
      "        - **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
      "        - **encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "          Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n",
      "        - **encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
      "          self-attention heads.\n",
      "  \n",
      "    \n",
      "Summarization example:\n",
      "\n",
      "```python\n",
      ">>> from transformers import AutoTokenizer, BartForConditionalGeneration\n",
      "\n",
      ">>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
      "\n",
      ">>> ARTICLE_TO_SUMMARIZE = (\n",
      "...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
      "...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
      "...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
      "... )\n",
      ">>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n",
      "\n",
      ">>> # Generate Summary\n",
      ">>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n",
      ">>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'\n",
      "```\n",
      "\n",
      "Mask filling example:\n",
      "\n",
      "```python\n",
      ">>> from transformers import AutoTokenizer, BartForConditionalGeneration\n",
      "\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
      ">>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
      "\n",
      ">>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
      ">>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
      ">>> logits = model(input_ids).logits\n",
      "\n",
      ">>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
      ">>> probs = logits[0, masked_index].softmax(dim=0)\n",
      ">>> values, predictions = probs.topk(5)\n",
      "\n",
      ">>> tokenizer.decode(predictions).split()\n",
      "['not', 'good', 'healthy', 'great', 'very']\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model.forward?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
