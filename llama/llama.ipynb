{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama-repo'...\n",
      "remote: Enumerating objects: 12, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 12 (delta 0), reused 9 (delta 0), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (12/12), 16.73 KiB | 16.73 MiB/s, done.\n",
      "mv: cannot move 'llama-repo/llama' to './llama': Directory not empty\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:ypeleg/llama.git llama-repo && mv llama-repo/llama . && rm -rf llama-repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://test.pypi.org/simple/\n",
      "Collecting bitsandbytes\n",
      "  Downloading https://test-files.pythonhosted.org/packages/c2/9c/de2cf357ef756d76d4e9feca37e12303ae2472a3268b965b692dff2237b0/bitsandbytes-0.31.7-py3-none-any.whl (55.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "\u001b[33m  WARNING: The script debug_cuda is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.31.7\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b562a8081c483983b85da694bcbe14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad60a4359c5340e7ba457056233fa229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import llama\n",
    "\n",
    "MODEL = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "tokenizer = llama.LLaMATokenizer.from_pretrained(MODEL)\n",
    "model = llama.LLaMAForCausalLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry potter is a great example of a character who is not a hero, but is a hero\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "batch = tokenizer(\"Harry potter is a\", return_tensors = \"pt\")[\"input_ids\"].cuda()\n",
    "print(tokenizer.decode(model.generate(batch)[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
