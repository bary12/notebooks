{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/BeIR___json/BeIR--trec-news-generated-queries-58e8f34dd4c75682/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2b38478b9a4b5fa73eaf3a7a164882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('BeIR/trec-news-generated-queries')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/BeIR___json/BeIR--trec-news-generated-queries-58e8f34dd4c75682/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8f0959c31593cac5.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset['train'].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "documents = []\n",
    "titles = []\n",
    "for i in range(10000):\n",
    "    queries.append(dataset[i]['query'])\n",
    "    documents.append(dataset[i]['text'])\n",
    "    titles.append(dataset[i]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "\n",
    "#We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 512\n",
    "\n",
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a6bfeef5514835a3c000d169145306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "passages = []\n",
    "for i in range(len(documents)):\n",
    "    words = documents[i].split()\n",
    "    for j in range(0, len(words), 400):\n",
    "        passages.append({\"text\": \" \".join(words[j:j+400]), \"document_id\": i})\n",
    "encoded_passages = bi_encoder.encode(passages, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentences = []\n",
    "for i in range(len(documents)):\n",
    "    sents = nltk.sent_tokenize(documents[i])\n",
    "    sents = [{'text': s, 'document_id': i} for s in sents]\n",
    "    sentences.extend(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1ae830cd194174807d8bbfb0939aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_sentences = bi_encoder.encode([s['text'] for s in sentences], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c1779e7bd845a282d05e1bb9b54d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6659e3cc8743788d124e8b691d4b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/312717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# We lower case our text and remove stop-words from indexing\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm(passages):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage['text']))\n",
    "\n",
    "bm25_passages = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "tokenized_corpus = []\n",
    "for sentence in tqdm(sentences):\n",
    "    tokenized_corpus.append(bm25_tokenizer(sentence['text']))\n",
    "\n",
    "bm25_sentences = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, bm25, corpus, corpus_embeddings):\n",
    "    hits = []\n",
    "    num_bm_25 = 20\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -num_bm_25)[-num_bm_25:]\n",
    "    hits.extend(top_n)\n",
    "    num_bi_encoder = 20\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.cuda()\n",
    "    bi_encoder_hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=num_bi_encoder)\n",
    "    bi_encoder_hits = bi_encoder_hits[0]  # Get the hits for the first query\n",
    "    hits.extend(hit['corpus_id'] for hit in bi_encoder_hits)\n",
    "    hits = list(set(hits))\n",
    "    # cross-encode\n",
    "    cross_encoder_hits = cross_encoder.predict([(query, corpus[hit]['text']) for hit in hits])\n",
    "    hits = [hits[i] for i in np.argsort(cross_encoder_hits)[::-1]]\n",
    "    return hits[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage accuracy:  0.62\n",
      "Sentence accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "ac_passages = 0\n",
    "ac_sentences = 0\n",
    "samples = 100\n",
    "for i in range(samples):\n",
    "    ret = search(queries[i], bm25_passages, passages, encoded_passages)\n",
    "    documents = [passages[hit]['document_id'] for hit in ret]\n",
    "    if i in documents:\n",
    "        ac_passages += 1\n",
    "    ret = search(queries[i], bm25_sentences, sentences, encoded_sentences)\n",
    "    documents = [sentences[hit]['document_id'] for hit in ret]\n",
    "    if i in documents:\n",
    "        ac_sentences += 1\n",
    "\n",
    "print('Passage accuracy: ', ac_passages/samples)\n",
    "print('Sentence accuracy: ', ac_sentences/samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
