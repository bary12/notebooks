{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class SelfAttentionHead(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, query_dim, key_dim, value_dim):\n",
    "        super().__init__()\n",
    "        self.query_dim = query_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.Wq = torch.nn.Linear(embedding_dim, query_dim, bias=False)\n",
    "        self.Wk = torch.nn.Linear(embedding_dim, key_dim, bias=False)\n",
    "        self.Wv = torch.nn.Linear(embedding_dim, value_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = torch.matmul(x, torch.transpose(self.Wq.weight, 0, 1))\n",
    "        k = torch.matmul(x, torch.transpose(self.Wk.weight, 0, 1))\n",
    "        v = torch.matmul(x, torch.transpose(self.Wv.weight.data, 0, 1))\n",
    "\n",
    "        energy = torch.matmul(q, k.transpose(1, 2))\n",
    "        normalized_energy = torch.softmax(energy / math.sqrt(self.key_dim), dim=2)\n",
    "        out = torch.matmul(normalized_energy, v)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, nheads, embedding_dim, query_dim, key_dim, value_dim):\n",
    "        super().__init__()\n",
    "        self.attention_heads = torch.nn.ModuleList([\n",
    "            SelfAttentionHead(embedding_dim, query_dim, key_dim, value_dim)\n",
    "            for _ in range(nheads)\n",
    "        ])\n",
    "        self.Wo = torch.nn.Linear(nheads * value_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat(tuple(ah(x) for ah in self.attention_heads), dim=2)\n",
    "        output = self.Wo(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, nheads, embedding_dim, query_dim, key_dim, value_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(nheads, embedding_dim, query_dim, key_dim, value_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fully_connected = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embedding_dim * 4, embedding_dim * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embedding_dim * 4, embedding_dim),\n",
    "        )\n",
    "\n",
    "        self.norm2 = torch.nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.multi_head_attention(x)\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.fully_connected(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, n_tokens, num_encoder_layers, nheads, embedding_dim, query_dim, key_dim, value_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = torch.nn.Embedding(n_tokens, embedding_dim)\n",
    "        self.encoder_layers = torch.nn.ModuleList([\n",
    "            EncoderLayer(nheads, embedding_dim, query_dim, key_dim, value_dim)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.pe = PositionalEncoding(d_model=embedding_dim, max_len=embedding_dim // 2)\n",
    "        self.classifier = torch.nn.Linear(max_len * embedding_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('applying embedding to:', x.shape)\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding_dim)\n",
    "        # print('applying pos encoding to:', x.shape)\n",
    "        x = self.pe(x)\n",
    "        # print('applying encoder layers to:', x.shape)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        # print('applying classifier to:', x.shape)\n",
    "        x = self.classifier(x.flatten(start_dim=1))\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c476a301f0547e5b9c03e6afc2ed297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-01ad04b69ceba701.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-f85c25fbaaf4c75e.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-bfdb1e053999b3b1.arrow\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "tok = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "ntokens = len(tok.get_vocab().keys())\n",
    "model = Encoder(ntokens, num_encoder_layers=3, nheads=8, embedding_dim=200, query_dim=64, key_dim=64, value_dim=64, max_len=256)\n",
    "\n",
    "\n",
    "data = datasets.load_dataset('imdb').with_format('torch').shuffle(seed=42)\n",
    "train_data = data['train']\n",
    "test_data = data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-04)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "MAX = 16\n",
    "\n",
    "def train(model: torch.nn.Module, epochs: int) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.\n",
    "        log_interval = 1\n",
    "        batch_size = 5\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(0, MAX, batch_size):\n",
    "            batch = train_data[i:i+batch_size]\n",
    "            tokenized = tok.batch_encode_plus(batch['text'], max_length=256,\n",
    "                                            padding='max_length', truncation=True, return_tensors='pt')['input_ids']\n",
    "            output = model(tokenized)\n",
    "            oh = torch.nn.functional.one_hot(batch['label'], num_classes=2).float()\n",
    "            loss = criterion(output, oh)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i // batch_size) % log_interval == 0 and i > 0:\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "                cur_loss = total_loss / log_interval\n",
    "                ppl = math.exp(cur_loss)\n",
    "                print(f'| epoch {epoch:3d} | {i // batch_size:5.0f}/{MAX//batch_size:5.0f} batches | '\n",
    "                    f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                    f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "        print('======')\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     1/    3 batches | lr 0.00 | ms/batch 1053.31 | loss  1.31 | ppl     3.70\n",
      "| epoch   1 |     2/    3 batches | lr 0.00 | ms/batch 517.44 | loss  0.73 | ppl     2.09\n",
      "| epoch   1 |     3/    3 batches | lr 0.00 | ms/batch 517.33 | loss  0.71 | ppl     2.03\n",
      "======\n",
      "| epoch   2 |     1/    3 batches | lr 0.00 | ms/batch 1041.34 | loss  0.63 | ppl     1.88\n",
      "| epoch   2 |     2/    3 batches | lr 0.00 | ms/batch 525.41 | loss  0.38 | ppl     1.46\n",
      "| epoch   2 |     3/    3 batches | lr 0.00 | ms/batch 518.73 | loss  0.34 | ppl     1.41\n",
      "======\n",
      "| epoch   3 |     1/    3 batches | lr 0.00 | ms/batch 1059.99 | loss  0.63 | ppl     1.87\n",
      "| epoch   3 |     2/    3 batches | lr 0.00 | ms/batch 515.86 | loss  0.33 | ppl     1.39\n",
      "| epoch   3 |     3/    3 batches | lr 0.00 | ms/batch 515.93 | loss  0.32 | ppl     1.37\n",
      "======\n",
      "| epoch   4 |     1/    3 batches | lr 0.00 | ms/batch 1033.88 | loss  0.63 | ppl     1.87\n",
      "| epoch   4 |     2/    3 batches | lr 0.00 | ms/batch 521.93 | loss  0.31 | ppl     1.37\n",
      "| epoch   4 |     3/    3 batches | lr 0.00 | ms/batch 516.26 | loss  0.31 | ppl     1.37\n",
      "======\n",
      "| epoch   5 |     1/    3 batches | lr 0.00 | ms/batch 1036.51 | loss  0.63 | ppl     1.87\n",
      "| epoch   5 |     2/    3 batches | lr 0.00 | ms/batch 515.07 | loss  0.31 | ppl     1.37\n",
      "| epoch   5 |     3/    3 batches | lr 0.00 | ms/batch 516.06 | loss  0.31 | ppl     1.37\n",
      "======\n",
      "| epoch   6 |     1/    3 batches | lr 0.00 | ms/batch 1034.46 | loss  0.63 | ppl     1.87\n",
      "| epoch   6 |     2/    3 batches | lr 0.00 | ms/batch 517.09 | loss  0.31 | ppl     1.37\n",
      "| epoch   6 |     3/    3 batches | lr 0.00 | ms/batch 523.49 | loss  0.31 | ppl     1.37\n",
      "======\n",
      "| epoch   7 |     1/    3 batches | lr 0.00 | ms/batch 1043.08 | loss  0.63 | ppl     1.87\n",
      "| epoch   7 |     2/    3 batches | lr 0.00 | ms/batch 516.79 | loss  0.31 | ppl     1.37\n",
      "| epoch   7 |     3/    3 batches | lr 0.00 | ms/batch 529.02 | loss  0.31 | ppl     1.37\n",
      "======\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, \u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[28], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, oh)\n\u001b[1;32m     23\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     25\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m0.5\u001b[39m)\n\u001b[1;32m     26\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized = tok.encode_plus(test_data[0]['text'], max_length=256,\n",
    "                                  padding='max_length', truncation=True, return_tensors='pt')['input_ids']\n",
    "print(tokenized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (50) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predicted \u001b[39m=\u001b[39m model(tokenized)\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 113\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m    112\u001b[0m \u001b[39m# print('applying pos encoding to:', x.shape)\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpe(x)\n\u001b[1;32m    114\u001b[0m \u001b[39m# print('applying encoder layers to:', x.shape)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layers:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 93\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     89\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m        x: Tensor, shape [seq_len, batch_size, embedding_dim]\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpe[:x\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)]\n\u001b[1;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (50) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "predicted = model(tokenized).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = torch.tensor([test_data[i]['label'].item() for i in range(0, 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(47)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(52)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predicted == actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/ubuntu/models/my-transformer-trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Encoder' object has no attribute 'encoder_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[256], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[1;32m      2\u001b[0m     tokenized \u001b[39m=\u001b[39m tok\u001b[39m.\u001b[39mbatch_encode_plus([test_data[i][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]], max_length\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                     padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mprint\u001b[39m(model(tokenized)\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), test_data[i][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[231], line 113\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m    112\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpe(x)\n\u001b[0;32m--> 113\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_layers:\n\u001b[1;32m    114\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m    115\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Encoder' object has no attribute 'encoder_layers'"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    tokenized = tok.batch_encode_plus([test_data[i]['text']], max_length=256,\n",
    "                                    padding='max_length', truncation=True, return_tensors='pt')['input_ids']\n",
    "    print(model(tokenized).argmax(dim=1), test_data[i]['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
