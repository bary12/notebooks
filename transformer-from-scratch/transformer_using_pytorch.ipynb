{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "\n",
    "class SelfAttentionHead(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, query_dim, key_dim, value_dim):\n",
    "        super().__init__()\n",
    "        self.query_dim = query_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.Wq = torch.nn.Linear(embedding_dim, query_dim, bias=False)\n",
    "        self.Wk = torch.nn.Linear(embedding_dim, key_dim, bias=False)\n",
    "        self.Wv = torch.nn.Linear(embedding_dim, value_dim, bias=False)\n",
    "\n",
    "    called = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not SelfAttentionHead.called:\n",
    "            print('my forward called')\n",
    "            SelfAttentionHead.called = True\n",
    "            \n",
    "        q = torch.matmul(x, torch.transpose(self.Wq.weight, 0, 1))\n",
    "        k = torch.matmul(x, torch.transpose(self.Wk.weight, 0, 1))\n",
    "        v = torch.matmul(x, torch.transpose(self.Wv.weight.data, 0, 1))\n",
    "\n",
    "        energy = torch.matmul(q, k.transpose(1, 2))\n",
    "        normalized_energy = torch.softmax(energy / math.sqrt(self.key_dim), dim=2)\n",
    "        out = torch.matmul(normalized_energy, v)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, nheads, embedding_dim, query_dim, key_dim, value_dim):\n",
    "        super().__init__()\n",
    "        self.attention_heads = torch.nn.ModuleList([\n",
    "            SelfAttentionHead(embedding_dim, query_dim, key_dim, value_dim)\n",
    "            for _ in range(nheads)\n",
    "        ])\n",
    "        self.Wo = torch.nn.Linear(nheads * value_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat(tuple(ah(x) for ah in self.attention_heads), dim=2)\n",
    "        output = self.Wo(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, nheads, embedding_dim, query_dim, key_dim, value_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(nheads, embedding_dim, query_dim, key_dim, value_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fully_connected = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embedding_dim * 4, embedding_dim * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(embedding_dim * 4, embedding_dim),\n",
    "        )\n",
    "\n",
    "        self.norm2 = torch.nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x, src_mask=None, src_key_padding_mask=None):\n",
    "        x = x + self.multi_head_attention(x)\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.fully_connected(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        # encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        encoder_layers = EncoderLayer(nheads=nhead, embedding_dim=d_model, query_dim=64, key_dim=64, value_dim=64)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.classifier = nn.Linear(d_model * 256, 2)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        src_mask = torch.ones(src.size(0), src.size(0), device=src.device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.classifier(output.flatten(start_dim=1))\n",
    "        return output\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, n_tokens, num_encoder_layers, nheads, embedding_dim, query_dim, key_dim, value_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = torch.nn.Embedding(n_tokens, embedding_dim)\n",
    "        self.encoder_layers = torch.nn.ModuleList([\n",
    "            EncoderLayer(nheads, embedding_dim, query_dim, key_dim, value_dim)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.pe = PositionalEncoding(d_model=embedding_dim, max_len=embedding_dim // 2)\n",
    "        self.classifier = torch.nn.Linear(embedding_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('applying embedding to:', x.shape)\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding_dim)\n",
    "        # print('applying pos encoding to:', x.shape)\n",
    "        x = self.pe(x)\n",
    "        # print('applying encoder layers to:', x.shape)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        # print('applying classifier to:', x.shape)\n",
    "        x = torch.softmax(x[0], dim=1)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453d0ba36bde4d70b437e548208d15ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-01ad04b69ceba701.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-f85c25fbaaf4c75e.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-bfdb1e053999b3b1.arrow\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "ntokens = len(tok.get_vocab().keys())\n",
    "model = TransformerModel(ntoken=ntokens, nlayers=2, nhead=4, d_model=48, d_hid=20)\n",
    "# model = Encoder(n_tokens=ntokens, num_encoder_layers=3, nheads=8, embedding_dim=200, query_dim=64, key_dim=64, value_dim=64, max_len=256)\n",
    "data = datasets.load_dataset('imdb').with_format('torch').shuffle(seed=42)\n",
    "train_data = data['train']\n",
    "test_data = data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def evaluate(model):\n",
    "    print('======')\n",
    "    print('Evaluating...')\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    examples = 1000\n",
    "    batch = 100\n",
    "    num_batches = 10\n",
    "    for i in range(0, examples, batch):\n",
    "        tokenized = tok.batch_encode_plus([test_data[i]['text'] for i in range(100)], max_length=256,\n",
    "                                    padding='max_length', truncation=True, return_tensors='pt')['input_ids']\n",
    "        result = model(tokenized)\n",
    "        actual = torch.tensor([test_data[i]['label'].item() for i in range(0, 100)])\n",
    "        actual_oh = torch.nn.functional.one_hot(actual, num_classes=2).float()\n",
    "        loss += torch.nn.CrossEntropyLoss()(result, actual_oh).item()\n",
    "        correct += sum(result.argmax(dim=1) == actual).item()\n",
    "    print('Loss: ', loss / num_batches)\n",
    "    print('Accuracy: ', correct * 100 / examples, '%')\n",
    "    print('======')\n",
    "\n",
    "MAX = 8001\n",
    "def train(model: torch.nn.Module, epochs: int) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.\n",
    "        log_interval = 25\n",
    "        batch_size = 10\n",
    "        eval_interval = 400\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(0, MAX, batch_size):\n",
    "            batch = train_data[i:i+batch_size]\n",
    "            tokenized = tok.batch_encode_plus(list(batch['text']), max_length=256,\n",
    "                                            padding='max_length', truncation=True, return_tensors='pt')['input_ids']\n",
    "            output = model(tokenized)\n",
    "            loss = criterion(output, torch.nn.functional.one_hot(batch['label'], num_classes=2).float())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_number = i // batch_size\n",
    "            if batch_number % log_interval == 0 and i > 0:\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "                cur_loss = total_loss / log_interval\n",
    "                ppl = math.exp(cur_loss)\n",
    "                print(f'| epoch {epoch:3d} | {batch_number:5.0f}/{MAX//batch_size:5.0f} batches | '\n",
    "                    f'lr {lr:02.6f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                    f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "            if batch_number % eval_interval == 0 and i > 0:\n",
    "                evaluate(model)\n",
    "                scheduler.step()\n",
    "\n",
    "        evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    25/  800 batches | lr 0.000015 | ms/batch 41.97 | loss  0.75 | ppl     2.12\n",
      "| epoch   1 |    50/  800 batches | lr 0.000015 | ms/batch 40.79 | loss  0.70 | ppl     2.02\n",
      "| epoch   1 |    75/  800 batches | lr 0.000015 | ms/batch 40.89 | loss  0.73 | ppl     2.07\n",
      "| epoch   1 |   100/  800 batches | lr 0.000015 | ms/batch 40.73 | loss  0.72 | ppl     2.06\n",
      "| epoch   1 |   125/  800 batches | lr 0.000015 | ms/batch 40.97 | loss  0.75 | ppl     2.11\n",
      "| epoch   1 |   150/  800 batches | lr 0.000015 | ms/batch 41.04 | loss  0.74 | ppl     2.09\n",
      "| epoch   1 |   175/  800 batches | lr 0.000015 | ms/batch 41.14 | loss  0.72 | ppl     2.06\n",
      "| epoch   1 |   200/  800 batches | lr 0.000015 | ms/batch 41.01 | loss  0.72 | ppl     2.05\n",
      "| epoch   1 |   225/  800 batches | lr 0.000015 | ms/batch 40.97 | loss  0.71 | ppl     2.04\n",
      "| epoch   1 |   250/  800 batches | lr 0.000015 | ms/batch 40.74 | loss  0.73 | ppl     2.09\n",
      "| epoch   1 |   275/  800 batches | lr 0.000015 | ms/batch 41.03 | loss  0.70 | ppl     2.01\n",
      "| epoch   1 |   300/  800 batches | lr 0.000015 | ms/batch 42.01 | loss  0.74 | ppl     2.09\n",
      "| epoch   1 |   325/  800 batches | lr 0.000015 | ms/batch 40.69 | loss  0.71 | ppl     2.04\n",
      "| epoch   1 |   350/  800 batches | lr 0.000015 | ms/batch 40.92 | loss  0.73 | ppl     2.07\n",
      "| epoch   1 |   375/  800 batches | lr 0.000015 | ms/batch 41.03 | loss  0.75 | ppl     2.12\n",
      "| epoch   1 |   400/  800 batches | lr 0.000015 | ms/batch 40.49 | loss  0.73 | ppl     2.07\n",
      "======\n",
      "Evaluating...\n",
      "Loss:  0.7476319015026093\n",
      "Accuracy:  0.49 %\n",
      "======\n",
      "| epoch   1 |   425/  800 batches | lr 0.000014 | ms/batch 294.47 | loss  0.72 | ppl     2.06\n",
      "| epoch   1 |   450/  800 batches | lr 0.000014 | ms/batch 41.16 | loss  0.75 | ppl     2.12\n",
      "| epoch   1 |   475/  800 batches | lr 0.000014 | ms/batch 41.12 | loss  0.71 | ppl     2.04\n",
      "| epoch   1 |   500/  800 batches | lr 0.000014 | ms/batch 41.04 | loss  0.71 | ppl     2.03\n",
      "| epoch   1 |   525/  800 batches | lr 0.000014 | ms/batch 40.64 | loss  0.75 | ppl     2.12\n",
      "| epoch   1 |   550/  800 batches | lr 0.000014 | ms/batch 40.68 | loss  0.77 | ppl     2.16\n",
      "| epoch   1 |   575/  800 batches | lr 0.000014 | ms/batch 41.89 | loss  0.72 | ppl     2.05\n",
      "| epoch   1 |   600/  800 batches | lr 0.000014 | ms/batch 40.46 | loss  0.72 | ppl     2.06\n",
      "| epoch   1 |   625/  800 batches | lr 0.000014 | ms/batch 41.10 | loss  0.71 | ppl     2.04\n",
      "| epoch   1 |   650/  800 batches | lr 0.000014 | ms/batch 41.54 | loss  0.71 | ppl     2.02\n",
      "| epoch   1 |   675/  800 batches | lr 0.000014 | ms/batch 40.45 | loss  0.73 | ppl     2.08\n",
      "| epoch   1 |   700/  800 batches | lr 0.000014 | ms/batch 41.69 | loss  0.72 | ppl     2.06\n",
      "| epoch   1 |   725/  800 batches | lr 0.000014 | ms/batch 40.89 | loss  0.71 | ppl     2.04\n",
      "| epoch   1 |   750/  800 batches | lr 0.000014 | ms/batch 40.85 | loss  0.73 | ppl     2.08\n",
      "| epoch   1 |   775/  800 batches | lr 0.000014 | ms/batch 40.75 | loss  0.69 | ppl     1.99\n",
      "| epoch   1 |   800/  800 batches | lr 0.000014 | ms/batch 40.41 | loss  0.77 | ppl     2.16\n",
      "======\n",
      "Evaluating...\n",
      "Loss:  0.7217976570129394\n",
      "Accuracy:  0.528 %\n",
      "======\n",
      "======\n",
      "Evaluating...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, \u001b[39m3\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[110], line 65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         evaluate(model)\n\u001b[1;32m     63\u001b[0m         scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 65\u001b[0m evaluate(model)\n",
      "Cell \u001b[0;32mIn[110], line 17\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, examples, batch):\n\u001b[1;32m     15\u001b[0m     tokenized \u001b[39m=\u001b[39m tok\u001b[39m.\u001b[39mbatch_encode_plus([test_data[i][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m)], max_length\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,\n\u001b[1;32m     16\u001b[0m                                 padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m     result \u001b[39m=\u001b[39m model(tokenized)\n\u001b[1;32m     18\u001b[0m     actual \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([test_data[i][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m100\u001b[39m)])\n\u001b[1;32m     19\u001b[0m     actual_oh \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mone_hot(actual, num_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[97], line 112\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m    110\u001b[0m src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(src)\n\u001b[1;32m    111\u001b[0m src_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(src\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), src\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), device\u001b[39m=\u001b[39msrc\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 112\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(src, src_mask)\n\u001b[1;32m    113\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(output\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:280\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    277\u001b[0m         src_key_padding_mask_for_layers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 280\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    283\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:538\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    536\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    537\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    539\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    541\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:546\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    545\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 546\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    547\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    548\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    549\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1157\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1158\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1165\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1168\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1169\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1172\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1173\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1174\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:5163\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5161\u001b[0m attn_output_weights \u001b[39m=\u001b[39m softmax(attn_output_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   5162\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m-> 5163\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m dropout(attn_output_weights, p\u001b[39m=\u001b[39;49mdropout_p)\n\u001b[1;32m   5165\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(attn_output_weights, v)\n\u001b[1;32m   5167\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len \u001b[39m*\u001b[39m bsz, embed_dim)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "        1, 1, 0, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
