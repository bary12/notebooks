{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "\n",
    "#We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 512     #Truncate long passages to 256 tokens\n",
    "\n",
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 2 * encoding_dim\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim * 2, self.hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim * 2, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, query, passage):\n",
    "        x = torch.cat([query, passage], dim=-1)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "    \n",
    "cross_embeddings = torch.load('~/models/cross-embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/BeIR___json/BeIR--trec-news-generated-queries-58e8f34dd4c75682/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/BeIR___json/BeIR--trec-news-generated-queries-58e8f34dd4c75682/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8f0959c31593cac5.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('BeIR/trec-news-generated-queries', split='train').shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "passages = []\n",
    "titles = []\n",
    "for i in range(10000):\n",
    "    queries.append(dataset[i]['query'])\n",
    "    passages.append(dataset[i]['text'])\n",
    "    titles.append(dataset[i]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daa944421304fd5bdf8b5bb4d6ebd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, top_k=3):\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.cuda()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=100)\n",
    "    \n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "    embeds = torch.stack([corpus_embeddings[hit['corpus_id']] for hit in hits])\n",
    "    q_embeds = question_embedding.unsqueeze(0).repeat(len(embeds), 1)\n",
    "\n",
    "    output = cross_embeddings(q_embeds, embeds)\n",
    "    for idx in range(len(output)):\n",
    "        hits[idx]['cross_score'] = output[idx]\n",
    "\n",
    "    hits = sorted(hits, key=lambda x: x['cross_score'], reverse=True)\n",
    "    return hits[0:top_k]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@3: 0.03\n"
     ]
    }
   ],
   "source": [
    "n = 200\n",
    "k = 3\n",
    "recall_at_k = 0\n",
    "for i in range(n):\n",
    "    results = search(queries[i], top_k=k)\n",
    "    if any(result['corpus_id'] == i for result in results):\n",
    "        recall_at_k += 1\n",
    "print(\"Recall@{}: {}\".format(k, recall_at_k/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
