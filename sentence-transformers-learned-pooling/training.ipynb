{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 21:42:46 - Use pytorch device: cuda\n",
      "2023-03-07 21:42:46 - Read AllNLI train dataset\n",
      "2023-03-07 21:42:59 - Train samples: 563648\n",
      "2023-03-07 21:42:59 - Read STSbenchmark dev dataset\n",
      "2023-03-07 21:42:59 - Warmup-steps: 441\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c049d75340847ad9ae59c9983f0fe9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cb856449764d06940ccf1dc667c7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/4403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 21:43:19 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 44 steps:\n",
      "2023-03-07 21:43:19 - Cosine-Similarity :\tPearson: 0.3022\tSpearman: 0.2691\n",
      "2023-03-07 21:43:19 - Manhattan-Distance:\tPearson: 0.2032\tSpearman: 0.2042\n",
      "2023-03-07 21:43:19 - Euclidean-Distance:\tPearson: 0.2058\tSpearman: 0.2059\n",
      "2023-03-07 21:43:19 - Dot-Product-Similarity:\tPearson: 0.0474\tSpearman: 0.0996\n",
      "2023-03-07 21:43:19 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:43:37 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 88 steps:\n",
      "2023-03-07 21:43:38 - Cosine-Similarity :\tPearson: 0.7053\tSpearman: 0.7462\n",
      "2023-03-07 21:43:38 - Manhattan-Distance:\tPearson: 0.5854\tSpearman: 0.5841\n",
      "2023-03-07 21:43:38 - Euclidean-Distance:\tPearson: 0.5854\tSpearman: 0.5846\n",
      "2023-03-07 21:43:38 - Dot-Product-Similarity:\tPearson: 0.3452\tSpearman: 0.3652\n",
      "2023-03-07 21:43:38 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:43:57 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 132 steps:\n",
      "2023-03-07 21:43:58 - Cosine-Similarity :\tPearson: 0.7729\tSpearman: 0.7946\n",
      "2023-03-07 21:43:58 - Manhattan-Distance:\tPearson: 0.6301\tSpearman: 0.6349\n",
      "2023-03-07 21:43:58 - Euclidean-Distance:\tPearson: 0.6302\tSpearman: 0.6344\n",
      "2023-03-07 21:43:58 - Dot-Product-Similarity:\tPearson: 0.3539\tSpearman: 0.4006\n",
      "2023-03-07 21:43:58 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:44:18 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 176 steps:\n",
      "2023-03-07 21:44:18 - Cosine-Similarity :\tPearson: 0.7868\tSpearman: 0.8091\n",
      "2023-03-07 21:44:18 - Manhattan-Distance:\tPearson: 0.6509\tSpearman: 0.6571\n",
      "2023-03-07 21:44:18 - Euclidean-Distance:\tPearson: 0.6514\tSpearman: 0.6572\n",
      "2023-03-07 21:44:18 - Dot-Product-Similarity:\tPearson: 0.4040\tSpearman: 0.4838\n",
      "2023-03-07 21:44:18 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:44:38 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 220 steps:\n",
      "2023-03-07 21:44:39 - Cosine-Similarity :\tPearson: 0.8039\tSpearman: 0.8237\n",
      "2023-03-07 21:44:39 - Manhattan-Distance:\tPearson: 0.6569\tSpearman: 0.6639\n",
      "2023-03-07 21:44:39 - Euclidean-Distance:\tPearson: 0.6589\tSpearman: 0.6651\n",
      "2023-03-07 21:44:39 - Dot-Product-Similarity:\tPearson: 0.4366\tSpearman: 0.5086\n",
      "2023-03-07 21:44:39 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:44:59 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 264 steps:\n",
      "2023-03-07 21:44:59 - Cosine-Similarity :\tPearson: 0.8000\tSpearman: 0.8214\n",
      "2023-03-07 21:44:59 - Manhattan-Distance:\tPearson: 0.6556\tSpearman: 0.6643\n",
      "2023-03-07 21:44:59 - Euclidean-Distance:\tPearson: 0.6569\tSpearman: 0.6651\n",
      "2023-03-07 21:44:59 - Dot-Product-Similarity:\tPearson: 0.4278\tSpearman: 0.5232\n",
      "2023-03-07 21:45:17 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 308 steps:\n",
      "2023-03-07 21:45:18 - Cosine-Similarity :\tPearson: 0.8066\tSpearman: 0.8277\n",
      "2023-03-07 21:45:18 - Manhattan-Distance:\tPearson: 0.6687\tSpearman: 0.6770\n",
      "2023-03-07 21:45:18 - Euclidean-Distance:\tPearson: 0.6698\tSpearman: 0.6776\n",
      "2023-03-07 21:45:18 - Dot-Product-Similarity:\tPearson: 0.4518\tSpearman: 0.5242\n",
      "2023-03-07 21:45:18 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:45:37 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 352 steps:\n",
      "2023-03-07 21:45:38 - Cosine-Similarity :\tPearson: 0.8105\tSpearman: 0.8323\n",
      "2023-03-07 21:45:38 - Manhattan-Distance:\tPearson: 0.6720\tSpearman: 0.6792\n",
      "2023-03-07 21:45:38 - Euclidean-Distance:\tPearson: 0.6730\tSpearman: 0.6798\n",
      "2023-03-07 21:45:38 - Dot-Product-Similarity:\tPearson: 0.4419\tSpearman: 0.5075\n",
      "2023-03-07 21:45:38 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:45:57 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 396 steps:\n",
      "2023-03-07 21:45:58 - Cosine-Similarity :\tPearson: 0.8162\tSpearman: 0.8360\n",
      "2023-03-07 21:45:58 - Manhattan-Distance:\tPearson: 0.6627\tSpearman: 0.6727\n",
      "2023-03-07 21:45:58 - Euclidean-Distance:\tPearson: 0.6646\tSpearman: 0.6742\n",
      "2023-03-07 21:45:58 - Dot-Product-Similarity:\tPearson: 0.4631\tSpearman: 0.5435\n",
      "2023-03-07 21:45:58 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:46:18 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 440 steps:\n",
      "2023-03-07 21:46:18 - Cosine-Similarity :\tPearson: 0.8060\tSpearman: 0.8288\n",
      "2023-03-07 21:46:18 - Manhattan-Distance:\tPearson: 0.6626\tSpearman: 0.6725\n",
      "2023-03-07 21:46:18 - Euclidean-Distance:\tPearson: 0.6635\tSpearman: 0.6730\n",
      "2023-03-07 21:46:18 - Dot-Product-Similarity:\tPearson: 0.4508\tSpearman: 0.5320\n",
      "2023-03-07 21:46:36 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 484 steps:\n",
      "2023-03-07 21:46:37 - Cosine-Similarity :\tPearson: 0.8178\tSpearman: 0.8371\n",
      "2023-03-07 21:46:37 - Manhattan-Distance:\tPearson: 0.6659\tSpearman: 0.6741\n",
      "2023-03-07 21:46:37 - Euclidean-Distance:\tPearson: 0.6668\tSpearman: 0.6747\n",
      "2023-03-07 21:46:37 - Dot-Product-Similarity:\tPearson: 0.4617\tSpearman: 0.5559\n",
      "2023-03-07 21:46:37 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:46:57 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 528 steps:\n",
      "2023-03-07 21:46:58 - Cosine-Similarity :\tPearson: 0.8167\tSpearman: 0.8377\n",
      "2023-03-07 21:46:58 - Manhattan-Distance:\tPearson: 0.6674\tSpearman: 0.6786\n",
      "2023-03-07 21:46:58 - Euclidean-Distance:\tPearson: 0.6685\tSpearman: 0.6794\n",
      "2023-03-07 21:46:58 - Dot-Product-Similarity:\tPearson: 0.4465\tSpearman: 0.5281\n",
      "2023-03-07 21:46:58 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:47:17 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 572 steps:\n",
      "2023-03-07 21:47:18 - Cosine-Similarity :\tPearson: 0.8150\tSpearman: 0.8361\n",
      "2023-03-07 21:47:18 - Manhattan-Distance:\tPearson: 0.6699\tSpearman: 0.6794\n",
      "2023-03-07 21:47:18 - Euclidean-Distance:\tPearson: 0.6708\tSpearman: 0.6802\n",
      "2023-03-07 21:47:18 - Dot-Product-Similarity:\tPearson: 0.4627\tSpearman: 0.5466\n",
      "2023-03-07 21:47:35 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 616 steps:\n",
      "2023-03-07 21:47:36 - Cosine-Similarity :\tPearson: 0.8197\tSpearman: 0.8391\n",
      "2023-03-07 21:47:36 - Manhattan-Distance:\tPearson: 0.6691\tSpearman: 0.6794\n",
      "2023-03-07 21:47:36 - Euclidean-Distance:\tPearson: 0.6697\tSpearman: 0.6798\n",
      "2023-03-07 21:47:36 - Dot-Product-Similarity:\tPearson: 0.4743\tSpearman: 0.5418\n",
      "2023-03-07 21:47:36 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:47:56 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 660 steps:\n",
      "2023-03-07 21:47:56 - Cosine-Similarity :\tPearson: 0.8258\tSpearman: 0.8471\n",
      "2023-03-07 21:47:56 - Manhattan-Distance:\tPearson: 0.6727\tSpearman: 0.6835\n",
      "2023-03-07 21:47:56 - Euclidean-Distance:\tPearson: 0.6739\tSpearman: 0.6846\n",
      "2023-03-07 21:47:56 - Dot-Product-Similarity:\tPearson: 0.4584\tSpearman: 0.5315\n",
      "2023-03-07 21:47:56 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:48:16 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 704 steps:\n",
      "2023-03-07 21:48:17 - Cosine-Similarity :\tPearson: 0.8239\tSpearman: 0.8433\n",
      "2023-03-07 21:48:17 - Manhattan-Distance:\tPearson: 0.6636\tSpearman: 0.6739\n",
      "2023-03-07 21:48:17 - Euclidean-Distance:\tPearson: 0.6648\tSpearman: 0.6748\n",
      "2023-03-07 21:48:17 - Dot-Product-Similarity:\tPearson: 0.4647\tSpearman: 0.5589\n",
      "2023-03-07 21:48:35 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 748 steps:\n",
      "2023-03-07 21:48:35 - Cosine-Similarity :\tPearson: 0.8299\tSpearman: 0.8482\n",
      "2023-03-07 21:48:35 - Manhattan-Distance:\tPearson: 0.6798\tSpearman: 0.6923\n",
      "2023-03-07 21:48:35 - Euclidean-Distance:\tPearson: 0.6809\tSpearman: 0.6932\n",
      "2023-03-07 21:48:35 - Dot-Product-Similarity:\tPearson: 0.4928\tSpearman: 0.5812\n",
      "2023-03-07 21:48:35 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:48:55 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 792 steps:\n",
      "2023-03-07 21:48:56 - Cosine-Similarity :\tPearson: 0.8274\tSpearman: 0.8451\n",
      "2023-03-07 21:48:56 - Manhattan-Distance:\tPearson: 0.6747\tSpearman: 0.6853\n",
      "2023-03-07 21:48:56 - Euclidean-Distance:\tPearson: 0.6764\tSpearman: 0.6866\n",
      "2023-03-07 21:48:56 - Dot-Product-Similarity:\tPearson: 0.4769\tSpearman: 0.5656\n",
      "2023-03-07 21:49:13 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 836 steps:\n",
      "2023-03-07 21:49:14 - Cosine-Similarity :\tPearson: 0.8305\tSpearman: 0.8485\n",
      "2023-03-07 21:49:14 - Manhattan-Distance:\tPearson: 0.6822\tSpearman: 0.6902\n",
      "2023-03-07 21:49:14 - Euclidean-Distance:\tPearson: 0.6832\tSpearman: 0.6913\n",
      "2023-03-07 21:49:14 - Dot-Product-Similarity:\tPearson: 0.4853\tSpearman: 0.5811\n",
      "2023-03-07 21:49:14 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:49:34 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 880 steps:\n",
      "2023-03-07 21:49:34 - Cosine-Similarity :\tPearson: 0.8318\tSpearman: 0.8504\n",
      "2023-03-07 21:49:34 - Manhattan-Distance:\tPearson: 0.6680\tSpearman: 0.6803\n",
      "2023-03-07 21:49:34 - Euclidean-Distance:\tPearson: 0.6694\tSpearman: 0.6816\n",
      "2023-03-07 21:49:34 - Dot-Product-Similarity:\tPearson: 0.4738\tSpearman: 0.5686\n",
      "2023-03-07 21:49:34 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:49:54 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 924 steps:\n",
      "2023-03-07 21:49:55 - Cosine-Similarity :\tPearson: 0.8315\tSpearman: 0.8481\n",
      "2023-03-07 21:49:55 - Manhattan-Distance:\tPearson: 0.6816\tSpearman: 0.6918\n",
      "2023-03-07 21:49:55 - Euclidean-Distance:\tPearson: 0.6828\tSpearman: 0.6927\n",
      "2023-03-07 21:49:55 - Dot-Product-Similarity:\tPearson: 0.5011\tSpearman: 0.5997\n",
      "2023-03-07 21:50:13 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 968 steps:\n",
      "2023-03-07 21:50:13 - Cosine-Similarity :\tPearson: 0.8221\tSpearman: 0.8416\n",
      "2023-03-07 21:50:13 - Manhattan-Distance:\tPearson: 0.6860\tSpearman: 0.6955\n",
      "2023-03-07 21:50:13 - Euclidean-Distance:\tPearson: 0.6876\tSpearman: 0.6972\n",
      "2023-03-07 21:50:13 - Dot-Product-Similarity:\tPearson: 0.4954\tSpearman: 0.5831\n",
      "2023-03-07 21:50:32 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1012 steps:\n",
      "2023-03-07 21:50:32 - Cosine-Similarity :\tPearson: 0.8314\tSpearman: 0.8485\n",
      "2023-03-07 21:50:32 - Manhattan-Distance:\tPearson: 0.6788\tSpearman: 0.6907\n",
      "2023-03-07 21:50:32 - Euclidean-Distance:\tPearson: 0.6800\tSpearman: 0.6917\n",
      "2023-03-07 21:50:32 - Dot-Product-Similarity:\tPearson: 0.5042\tSpearman: 0.5939\n",
      "2023-03-07 21:50:50 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1056 steps:\n",
      "2023-03-07 21:50:51 - Cosine-Similarity :\tPearson: 0.8333\tSpearman: 0.8514\n",
      "2023-03-07 21:50:51 - Manhattan-Distance:\tPearson: 0.6775\tSpearman: 0.6898\n",
      "2023-03-07 21:50:51 - Euclidean-Distance:\tPearson: 0.6786\tSpearman: 0.6907\n",
      "2023-03-07 21:50:51 - Dot-Product-Similarity:\tPearson: 0.4753\tSpearman: 0.5613\n",
      "2023-03-07 21:50:51 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:51:10 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1100 steps:\n",
      "2023-03-07 21:51:11 - Cosine-Similarity :\tPearson: 0.8366\tSpearman: 0.8546\n",
      "2023-03-07 21:51:11 - Manhattan-Distance:\tPearson: 0.6843\tSpearman: 0.6928\n",
      "2023-03-07 21:51:11 - Euclidean-Distance:\tPearson: 0.6855\tSpearman: 0.6940\n",
      "2023-03-07 21:51:11 - Dot-Product-Similarity:\tPearson: 0.4921\tSpearman: 0.5733\n",
      "2023-03-07 21:51:11 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:51:31 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1144 steps:\n",
      "2023-03-07 21:51:31 - Cosine-Similarity :\tPearson: 0.8382\tSpearman: 0.8523\n",
      "2023-03-07 21:51:31 - Manhattan-Distance:\tPearson: 0.6861\tSpearman: 0.6955\n",
      "2023-03-07 21:51:31 - Euclidean-Distance:\tPearson: 0.6870\tSpearman: 0.6963\n",
      "2023-03-07 21:51:31 - Dot-Product-Similarity:\tPearson: 0.4921\tSpearman: 0.5863\n",
      "2023-03-07 21:51:49 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1188 steps:\n",
      "2023-03-07 21:51:50 - Cosine-Similarity :\tPearson: 0.8402\tSpearman: 0.8535\n",
      "2023-03-07 21:51:50 - Manhattan-Distance:\tPearson: 0.6847\tSpearman: 0.6934\n",
      "2023-03-07 21:51:50 - Euclidean-Distance:\tPearson: 0.6857\tSpearman: 0.6946\n",
      "2023-03-07 21:51:50 - Dot-Product-Similarity:\tPearson: 0.4980\tSpearman: 0.5937\n",
      "2023-03-07 21:52:07 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1232 steps:\n",
      "2023-03-07 21:52:08 - Cosine-Similarity :\tPearson: 0.8413\tSpearman: 0.8553\n",
      "2023-03-07 21:52:08 - Manhattan-Distance:\tPearson: 0.6895\tSpearman: 0.6954\n",
      "2023-03-07 21:52:08 - Euclidean-Distance:\tPearson: 0.6910\tSpearman: 0.6970\n",
      "2023-03-07 21:52:08 - Dot-Product-Similarity:\tPearson: 0.4917\tSpearman: 0.5877\n",
      "2023-03-07 21:52:08 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:52:28 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1276 steps:\n",
      "2023-03-07 21:52:28 - Cosine-Similarity :\tPearson: 0.8462\tSpearman: 0.8604\n",
      "2023-03-07 21:52:28 - Manhattan-Distance:\tPearson: 0.6888\tSpearman: 0.6969\n",
      "2023-03-07 21:52:28 - Euclidean-Distance:\tPearson: 0.6899\tSpearman: 0.6977\n",
      "2023-03-07 21:52:28 - Dot-Product-Similarity:\tPearson: 0.5023\tSpearman: 0.5892\n",
      "2023-03-07 21:52:28 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-42-43\n",
      "2023-03-07 21:52:48 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1320 steps:\n",
      "2023-03-07 21:52:49 - Cosine-Similarity :\tPearson: 0.8423\tSpearman: 0.8554\n",
      "2023-03-07 21:52:49 - Manhattan-Distance:\tPearson: 0.6826\tSpearman: 0.6935\n",
      "2023-03-07 21:52:49 - Euclidean-Distance:\tPearson: 0.6835\tSpearman: 0.6941\n",
      "2023-03-07 21:52:49 - Dot-Product-Similarity:\tPearson: 0.5190\tSpearman: 0.6132\n",
      "2023-03-07 21:53:06 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1364 steps:\n",
      "2023-03-07 21:53:07 - Cosine-Similarity :\tPearson: 0.8422\tSpearman: 0.8562\n",
      "2023-03-07 21:53:07 - Manhattan-Distance:\tPearson: 0.6833\tSpearman: 0.6939\n",
      "2023-03-07 21:53:07 - Euclidean-Distance:\tPearson: 0.6840\tSpearman: 0.6943\n",
      "2023-03-07 21:53:07 - Dot-Product-Similarity:\tPearson: 0.5052\tSpearman: 0.5824\n",
      "2023-03-07 21:53:25 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1408 steps:\n",
      "2023-03-07 21:53:26 - Cosine-Similarity :\tPearson: 0.8379\tSpearman: 0.8523\n",
      "2023-03-07 21:53:26 - Manhattan-Distance:\tPearson: 0.6890\tSpearman: 0.6977\n",
      "2023-03-07 21:53:26 - Euclidean-Distance:\tPearson: 0.6899\tSpearman: 0.6983\n",
      "2023-03-07 21:53:26 - Dot-Product-Similarity:\tPearson: 0.5136\tSpearman: 0.5948\n",
      "2023-03-07 21:53:43 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1452 steps:\n",
      "2023-03-07 21:53:44 - Cosine-Similarity :\tPearson: 0.8335\tSpearman: 0.8491\n",
      "2023-03-07 21:53:44 - Manhattan-Distance:\tPearson: 0.6802\tSpearman: 0.6914\n",
      "2023-03-07 21:53:44 - Euclidean-Distance:\tPearson: 0.6812\tSpearman: 0.6922\n",
      "2023-03-07 21:53:44 - Dot-Product-Similarity:\tPearson: 0.5177\tSpearman: 0.6073\n",
      "2023-03-07 21:54:02 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1496 steps:\n",
      "2023-03-07 21:54:03 - Cosine-Similarity :\tPearson: 0.8420\tSpearman: 0.8567\n",
      "2023-03-07 21:54:03 - Manhattan-Distance:\tPearson: 0.6999\tSpearman: 0.7076\n",
      "2023-03-07 21:54:03 - Euclidean-Distance:\tPearson: 0.7006\tSpearman: 0.7082\n",
      "2023-03-07 21:54:03 - Dot-Product-Similarity:\tPearson: 0.5351\tSpearman: 0.6147\n",
      "2023-03-07 21:54:21 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1540 steps:\n",
      "2023-03-07 21:54:21 - Cosine-Similarity :\tPearson: 0.8429\tSpearman: 0.8573\n",
      "2023-03-07 21:54:21 - Manhattan-Distance:\tPearson: 0.6962\tSpearman: 0.7042\n",
      "2023-03-07 21:54:21 - Euclidean-Distance:\tPearson: 0.6970\tSpearman: 0.7049\n",
      "2023-03-07 21:54:21 - Dot-Product-Similarity:\tPearson: 0.5245\tSpearman: 0.6018\n",
      "2023-03-07 21:54:39 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1584 steps:\n",
      "2023-03-07 21:54:39 - Cosine-Similarity :\tPearson: 0.8423\tSpearman: 0.8568\n",
      "2023-03-07 21:54:39 - Manhattan-Distance:\tPearson: 0.6919\tSpearman: 0.7023\n",
      "2023-03-07 21:54:39 - Euclidean-Distance:\tPearson: 0.6923\tSpearman: 0.7027\n",
      "2023-03-07 21:54:39 - Dot-Product-Similarity:\tPearson: 0.5194\tSpearman: 0.6034\n",
      "2023-03-07 21:54:57 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1628 steps:\n",
      "2023-03-07 21:54:58 - Cosine-Similarity :\tPearson: 0.8459\tSpearman: 0.8603\n",
      "2023-03-07 21:54:58 - Manhattan-Distance:\tPearson: 0.6953\tSpearman: 0.7050\n",
      "2023-03-07 21:54:58 - Euclidean-Distance:\tPearson: 0.6957\tSpearman: 0.7053\n",
      "2023-03-07 21:54:58 - Dot-Product-Similarity:\tPearson: 0.5189\tSpearman: 0.6016\n",
      "2023-03-07 21:55:16 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1672 steps:\n",
      "2023-03-07 21:55:16 - Cosine-Similarity :\tPearson: 0.8441\tSpearman: 0.8576\n",
      "2023-03-07 21:55:16 - Manhattan-Distance:\tPearson: 0.6907\tSpearman: 0.7008\n",
      "2023-03-07 21:55:16 - Euclidean-Distance:\tPearson: 0.6914\tSpearman: 0.7013\n",
      "2023-03-07 21:55:16 - Dot-Product-Similarity:\tPearson: 0.5186\tSpearman: 0.5987\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">140 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">141 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">142 # Train the model</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>143 model.fit(train_objectives=[(train_dataloader, train_loss)],                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">144 │   │     </span>evaluator=dev_evaluator,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">145 │   │     </span>epochs=num_epochs,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146 │   │     </span>evaluation_steps=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(train_dataloader)*<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.01</span>),                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ubuntu/.local/lib/python3.10/site-packages/sentence_transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">SentenceTransformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">72</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fit</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">719 │   │   │   │   │   │   </span>skip_scheduler = scaler.get_scale() != scale_before_step           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">720 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">721 │   │   │   │   │   │   </span>loss_value = loss_model(features, labels)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>722 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>loss_value.backward()                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">723 │   │   │   │   │   │   </span>torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">724 │   │   │   │   │   │   </span>optimizer.step()                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">725 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ubuntu/.local/lib/python3.10/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">488</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 487 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 488 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 491 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">197</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">194 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>197 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m141 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m142 \u001b[0m\u001b[2m# Train the model\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m143 model.fit(train_objectives=[(train_dataloader, train_loss)],                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m144 \u001b[0m\u001b[2m│   │     \u001b[0mevaluator=dev_evaluator,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m145 \u001b[0m\u001b[2m│   │     \u001b[0mepochs=num_epochs,                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m146 \u001b[0m\u001b[2m│   │     \u001b[0mevaluation_steps=\u001b[96mint\u001b[0m(\u001b[96mlen\u001b[0m(train_dataloader)*\u001b[94m0.01\u001b[0m),                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ubuntu/.local/lib/python3.10/site-packages/sentence_transformers/\u001b[0m\u001b[1;33mSentenceTransformer.py\u001b[0m:\u001b[94m72\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m2\u001b[0m in \u001b[92mfit\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m719 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mskip_scheduler = scaler.get_scale() != scale_before_step           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m720 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m721 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mloss_value = loss_model(features, labels)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m722 \u001b[2m│   │   │   │   │   │   \u001b[0mloss_value.backward()                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m723 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtorch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m724 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0moptimizer.step()                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m725 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ubuntu/.local/lib/python3.10/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m488\u001b[0m in \u001b[92mbackward\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 487 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 488 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 491 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m197\u001b[0m in \u001b[92mbackward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m194 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m197 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The system trains BERT (or any other transformer model like RoBERTa, DistilBERT etc.) on the SNLI + MultiNLI (AllNLI) dataset\n",
    "with MultipleNegativesRankingLoss. Entailnments are poisitive pairs and the contradiction on AllNLI dataset is added as a hard negative.\n",
    "At every 10% training steps, the model is evaluated on the STS benchmark dataset\n",
    "Usage:\n",
    "python training_nli_v2.py\n",
    "OR\n",
    "python training_nli_v2.py pretrained_transformer_model_name\n",
    "\"\"\"\n",
    "import math\n",
    "from sentence_transformers import models, losses, datasets\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "model_name = 'distilroberta-base'\n",
    "train_batch_size = 128          #The larger you select this, the better the results (usually). But it requires more GPU memory\n",
    "max_seq_length = 75\n",
    "num_epochs = 1\n",
    "\n",
    "# Save path of the model\n",
    "model_save_path = 'output/training_nli_v2_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "class LearnedPooling(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LearnedPooling, self).__init__()\n",
    "        self.dense = torch.nn.Linear(75 * 768, 768, )\n",
    "        self.dense.weight.data.normal_(mean=1.0 / (75 * 768), std=1.0 / (75 * 768))\n",
    "        self.dense.bias.data.normal_(mean=0, std=1.0 / (75 * 768))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        #features: [batch_size, num_tokens, hidden_size]\n",
    "        #output: [batch_size, hidden_size]\n",
    "        x = features['token_embeddings']\n",
    "        attention_mask = features['attention_mask']\n",
    "        x = x * attention_mask.unsqueeze(-1).float()\n",
    "        size = x.size(1)\n",
    "        if size < 75:\n",
    "            x = torch.nn.functional.pad(x, (0, 0, 0, 75 - size, 0, 0), \"constant\", value=0)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #x: [batch_size, hidden_size]\n",
    "        #output: [batch_size, hidden_size]\n",
    "        return {'sentence_embedding': x}\n",
    "    \n",
    "    def save(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "# Here we define our SentenceTransformer model\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "pooling_model = LearnedPooling()\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "#Check if dataset exsist. If not, download and extract  it\n",
    "nli_dataset_path = 'data/AllNLI.tsv.gz'\n",
    "sts_dataset_path = 'data/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
    "\n",
    "\n",
    "# Read the AllNLI.tsv.gz file and create the training dataset\n",
    "logging.info(\"Read AllNLI train dataset\")\n",
    "\n",
    "def add_to_samples(sent1, sent2, label):\n",
    "    if sent1 not in train_data:\n",
    "        train_data[sent1] = {'contradiction': set(), 'entailment': set(), 'neutral': set()}\n",
    "    train_data[sent1][label].add(sent2)\n",
    "\n",
    "\n",
    "train_data = {}\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'train':\n",
    "            sent1 = row['sentence1'].strip()\n",
    "            sent2 = row['sentence2'].strip()\n",
    "\n",
    "            add_to_samples(sent1, sent2, row['label'])\n",
    "            add_to_samples(sent2, sent1, row['label'])  #Also add the opposite\n",
    "\n",
    "\n",
    "train_samples = []\n",
    "for sent1, others in train_data.items():\n",
    "    if len(others['entailment']) > 0 and len(others['contradiction']) > 0:\n",
    "        train_samples.append(InputExample(texts=[sent1, random.choice(list(others['entailment'])), random.choice(list(others['contradiction']))]))\n",
    "        train_samples.append(InputExample(texts=[random.choice(list(others['entailment'])), sent1, random.choice(list(others['contradiction']))]))\n",
    "\n",
    "logging.info(\"Train samples: {}\".format(len(train_samples)))\n",
    "\n",
    "\n",
    "\n",
    "# Special data loader that avoid duplicates within a batch\n",
    "train_dataloader = datasets.NoDuplicatesDataLoader(train_samples, batch_size=train_batch_size)\n",
    "\n",
    "\n",
    "# Our training loss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Read STSbenchmark dataset and use it as development set\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "dev_samples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'dev':\n",
    "            score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
    "            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, batch_size=train_batch_size, name='sts-dev')\n",
    "\n",
    "# Configure the training\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=dev_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=int(len(train_dataloader)*0.01),\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path,\n",
    "          use_amp=False          #Set to True, if your GPU supports FP16 operations\n",
    "          )\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "test_samples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'test':\n",
    "            score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
    "            test_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, batch_size=train_batch_size, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
