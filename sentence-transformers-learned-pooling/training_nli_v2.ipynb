{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 21:59:08 - Use pytorch device: cuda\n",
      "2023-03-07 21:59:08 - Read AllNLI train dataset\n",
      "2023-03-07 21:59:21 - Train samples: 563648\n",
      "2023-03-07 21:59:21 - Read STSbenchmark dev dataset\n",
      "2023-03-07 21:59:21 - Warmup-steps: 441\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba62ccaf8ae47b3b73c4f6892174c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50475aad806646b2a717d6318052a483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/4403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 21:59:40 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 44 steps:\n",
      "2023-03-07 21:59:40 - Cosine-Similarity :\tPearson: 0.7158\tSpearman: 0.7342\n",
      "2023-03-07 21:59:40 - Manhattan-Distance:\tPearson: 0.7539\tSpearman: 0.7562\n",
      "2023-03-07 21:59:40 - Euclidean-Distance:\tPearson: 0.7284\tSpearman: 0.7316\n",
      "2023-03-07 21:59:40 - Dot-Product-Similarity:\tPearson: 0.1827\tSpearman: 0.1709\n",
      "2023-03-07 21:59:40 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 21:59:58 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 88 steps:\n",
      "2023-03-07 21:59:59 - Cosine-Similarity :\tPearson: 0.7570\tSpearman: 0.7880\n",
      "2023-03-07 21:59:59 - Manhattan-Distance:\tPearson: 0.7634\tSpearman: 0.7681\n",
      "2023-03-07 21:59:59 - Euclidean-Distance:\tPearson: 0.7650\tSpearman: 0.7688\n",
      "2023-03-07 21:59:59 - Dot-Product-Similarity:\tPearson: 0.3687\tSpearman: 0.3626\n",
      "2023-03-07 21:59:59 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:00:18 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 132 steps:\n",
      "2023-03-07 22:00:19 - Cosine-Similarity :\tPearson: 0.8070\tSpearman: 0.8125\n",
      "2023-03-07 22:00:19 - Manhattan-Distance:\tPearson: 0.7955\tSpearman: 0.7951\n",
      "2023-03-07 22:00:19 - Euclidean-Distance:\tPearson: 0.7951\tSpearman: 0.7946\n",
      "2023-03-07 22:00:19 - Dot-Product-Similarity:\tPearson: 0.6210\tSpearman: 0.6352\n",
      "2023-03-07 22:00:19 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:00:37 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 176 steps:\n",
      "2023-03-07 22:00:38 - Cosine-Similarity :\tPearson: 0.8239\tSpearman: 0.8278\n",
      "2023-03-07 22:00:38 - Manhattan-Distance:\tPearson: 0.8070\tSpearman: 0.8065\n",
      "2023-03-07 22:00:38 - Euclidean-Distance:\tPearson: 0.8076\tSpearman: 0.8070\n",
      "2023-03-07 22:00:38 - Dot-Product-Similarity:\tPearson: 0.6728\tSpearman: 0.6837\n",
      "2023-03-07 22:00:38 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:00:57 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 220 steps:\n",
      "2023-03-07 22:00:57 - Cosine-Similarity :\tPearson: 0.8339\tSpearman: 0.8365\n",
      "2023-03-07 22:00:57 - Manhattan-Distance:\tPearson: 0.8151\tSpearman: 0.8140\n",
      "2023-03-07 22:00:57 - Euclidean-Distance:\tPearson: 0.8158\tSpearman: 0.8147\n",
      "2023-03-07 22:00:57 - Dot-Product-Similarity:\tPearson: 0.7124\tSpearman: 0.7157\n",
      "2023-03-07 22:00:57 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:01:16 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 264 steps:\n",
      "2023-03-07 22:01:17 - Cosine-Similarity :\tPearson: 0.8353\tSpearman: 0.8404\n",
      "2023-03-07 22:01:17 - Manhattan-Distance:\tPearson: 0.8168\tSpearman: 0.8159\n",
      "2023-03-07 22:01:17 - Euclidean-Distance:\tPearson: 0.8184\tSpearman: 0.8175\n",
      "2023-03-07 22:01:17 - Dot-Product-Similarity:\tPearson: 0.7167\tSpearman: 0.7199\n",
      "2023-03-07 22:01:17 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:01:37 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 308 steps:\n",
      "2023-03-07 22:01:37 - Cosine-Similarity :\tPearson: 0.8376\tSpearman: 0.8419\n",
      "2023-03-07 22:01:37 - Manhattan-Distance:\tPearson: 0.8234\tSpearman: 0.8221\n",
      "2023-03-07 22:01:37 - Euclidean-Distance:\tPearson: 0.8250\tSpearman: 0.8240\n",
      "2023-03-07 22:01:37 - Dot-Product-Similarity:\tPearson: 0.7346\tSpearman: 0.7351\n",
      "2023-03-07 22:01:37 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:01:56 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 352 steps:\n",
      "2023-03-07 22:01:56 - Cosine-Similarity :\tPearson: 0.8444\tSpearman: 0.8484\n",
      "2023-03-07 22:01:56 - Manhattan-Distance:\tPearson: 0.8247\tSpearman: 0.8241\n",
      "2023-03-07 22:01:56 - Euclidean-Distance:\tPearson: 0.8264\tSpearman: 0.8262\n",
      "2023-03-07 22:01:56 - Dot-Product-Similarity:\tPearson: 0.7481\tSpearman: 0.7500\n",
      "2023-03-07 22:01:56 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:02:16 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 396 steps:\n",
      "2023-03-07 22:02:16 - Cosine-Similarity :\tPearson: 0.8378\tSpearman: 0.8430\n",
      "2023-03-07 22:02:16 - Manhattan-Distance:\tPearson: 0.8211\tSpearman: 0.8204\n",
      "2023-03-07 22:02:16 - Euclidean-Distance:\tPearson: 0.8232\tSpearman: 0.8225\n",
      "2023-03-07 22:02:16 - Dot-Product-Similarity:\tPearson: 0.7330\tSpearman: 0.7347\n",
      "2023-03-07 22:02:33 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 440 steps:\n",
      "2023-03-07 22:02:34 - Cosine-Similarity :\tPearson: 0.8431\tSpearman: 0.8479\n",
      "2023-03-07 22:02:34 - Manhattan-Distance:\tPearson: 0.8291\tSpearman: 0.8284\n",
      "2023-03-07 22:02:34 - Euclidean-Distance:\tPearson: 0.8313\tSpearman: 0.8306\n",
      "2023-03-07 22:02:34 - Dot-Product-Similarity:\tPearson: 0.7298\tSpearman: 0.7299\n",
      "2023-03-07 22:02:50 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 484 steps:\n",
      "2023-03-07 22:02:51 - Cosine-Similarity :\tPearson: 0.8469\tSpearman: 0.8529\n",
      "2023-03-07 22:02:51 - Manhattan-Distance:\tPearson: 0.8297\tSpearman: 0.8287\n",
      "2023-03-07 22:02:51 - Euclidean-Distance:\tPearson: 0.8323\tSpearman: 0.8312\n",
      "2023-03-07 22:02:51 - Dot-Product-Similarity:\tPearson: 0.7503\tSpearman: 0.7527\n",
      "2023-03-07 22:02:51 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:03:10 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 528 steps:\n",
      "2023-03-07 22:03:11 - Cosine-Similarity :\tPearson: 0.8492\tSpearman: 0.8562\n",
      "2023-03-07 22:03:11 - Manhattan-Distance:\tPearson: 0.8330\tSpearman: 0.8319\n",
      "2023-03-07 22:03:11 - Euclidean-Distance:\tPearson: 0.8353\tSpearman: 0.8345\n",
      "2023-03-07 22:03:11 - Dot-Product-Similarity:\tPearson: 0.7552\tSpearman: 0.7557\n",
      "2023-03-07 22:03:11 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:03:30 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 572 steps:\n",
      "2023-03-07 22:03:30 - Cosine-Similarity :\tPearson: 0.8475\tSpearman: 0.8545\n",
      "2023-03-07 22:03:30 - Manhattan-Distance:\tPearson: 0.8394\tSpearman: 0.8376\n",
      "2023-03-07 22:03:30 - Euclidean-Distance:\tPearson: 0.8416\tSpearman: 0.8398\n",
      "2023-03-07 22:03:30 - Dot-Product-Similarity:\tPearson: 0.7736\tSpearman: 0.7691\n",
      "2023-03-07 22:03:47 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 616 steps:\n",
      "2023-03-07 22:03:48 - Cosine-Similarity :\tPearson: 0.8485\tSpearman: 0.8558\n",
      "2023-03-07 22:03:48 - Manhattan-Distance:\tPearson: 0.8371\tSpearman: 0.8354\n",
      "2023-03-07 22:03:48 - Euclidean-Distance:\tPearson: 0.8392\tSpearman: 0.8377\n",
      "2023-03-07 22:03:48 - Dot-Product-Similarity:\tPearson: 0.7598\tSpearman: 0.7568\n",
      "2023-03-07 22:04:06 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 660 steps:\n",
      "2023-03-07 22:04:06 - Cosine-Similarity :\tPearson: 0.8498\tSpearman: 0.8574\n",
      "2023-03-07 22:04:06 - Manhattan-Distance:\tPearson: 0.8404\tSpearman: 0.8394\n",
      "2023-03-07 22:04:06 - Euclidean-Distance:\tPearson: 0.8425\tSpearman: 0.8415\n",
      "2023-03-07 22:04:06 - Dot-Product-Similarity:\tPearson: 0.7846\tSpearman: 0.7828\n",
      "2023-03-07 22:04:06 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:04:25 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 704 steps:\n",
      "2023-03-07 22:04:26 - Cosine-Similarity :\tPearson: 0.8479\tSpearman: 0.8552\n",
      "2023-03-07 22:04:26 - Manhattan-Distance:\tPearson: 0.8374\tSpearman: 0.8368\n",
      "2023-03-07 22:04:26 - Euclidean-Distance:\tPearson: 0.8397\tSpearman: 0.8392\n",
      "2023-03-07 22:04:26 - Dot-Product-Similarity:\tPearson: 0.7678\tSpearman: 0.7663\n",
      "2023-03-07 22:04:43 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 748 steps:\n",
      "2023-03-07 22:04:44 - Cosine-Similarity :\tPearson: 0.8445\tSpearman: 0.8521\n",
      "2023-03-07 22:04:44 - Manhattan-Distance:\tPearson: 0.8373\tSpearman: 0.8361\n",
      "2023-03-07 22:04:44 - Euclidean-Distance:\tPearson: 0.8396\tSpearman: 0.8381\n",
      "2023-03-07 22:04:44 - Dot-Product-Similarity:\tPearson: 0.7557\tSpearman: 0.7548\n",
      "2023-03-07 22:05:01 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 792 steps:\n",
      "2023-03-07 22:05:01 - Cosine-Similarity :\tPearson: 0.8475\tSpearman: 0.8546\n",
      "2023-03-07 22:05:01 - Manhattan-Distance:\tPearson: 0.8422\tSpearman: 0.8409\n",
      "2023-03-07 22:05:01 - Euclidean-Distance:\tPearson: 0.8444\tSpearman: 0.8433\n",
      "2023-03-07 22:05:01 - Dot-Product-Similarity:\tPearson: 0.7727\tSpearman: 0.7704\n",
      "2023-03-07 22:05:19 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 836 steps:\n",
      "2023-03-07 22:05:19 - Cosine-Similarity :\tPearson: 0.8473\tSpearman: 0.8550\n",
      "2023-03-07 22:05:19 - Manhattan-Distance:\tPearson: 0.8420\tSpearman: 0.8402\n",
      "2023-03-07 22:05:19 - Euclidean-Distance:\tPearson: 0.8443\tSpearman: 0.8428\n",
      "2023-03-07 22:05:19 - Dot-Product-Similarity:\tPearson: 0.7720\tSpearman: 0.7691\n",
      "2023-03-07 22:05:36 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 880 steps:\n",
      "2023-03-07 22:05:37 - Cosine-Similarity :\tPearson: 0.8466\tSpearman: 0.8537\n",
      "2023-03-07 22:05:37 - Manhattan-Distance:\tPearson: 0.8378\tSpearman: 0.8371\n",
      "2023-03-07 22:05:37 - Euclidean-Distance:\tPearson: 0.8402\tSpearman: 0.8400\n",
      "2023-03-07 22:05:37 - Dot-Product-Similarity:\tPearson: 0.7727\tSpearman: 0.7724\n",
      "2023-03-07 22:05:54 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 924 steps:\n",
      "2023-03-07 22:05:55 - Cosine-Similarity :\tPearson: 0.8494\tSpearman: 0.8559\n",
      "2023-03-07 22:05:55 - Manhattan-Distance:\tPearson: 0.8377\tSpearman: 0.8362\n",
      "2023-03-07 22:05:55 - Euclidean-Distance:\tPearson: 0.8400\tSpearman: 0.8389\n",
      "2023-03-07 22:05:55 - Dot-Product-Similarity:\tPearson: 0.7721\tSpearman: 0.7708\n",
      "2023-03-07 22:06:12 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 968 steps:\n",
      "2023-03-07 22:06:13 - Cosine-Similarity :\tPearson: 0.8512\tSpearman: 0.8593\n",
      "2023-03-07 22:06:13 - Manhattan-Distance:\tPearson: 0.8465\tSpearman: 0.8452\n",
      "2023-03-07 22:06:13 - Euclidean-Distance:\tPearson: 0.8488\tSpearman: 0.8479\n",
      "2023-03-07 22:06:13 - Dot-Product-Similarity:\tPearson: 0.7994\tSpearman: 0.7983\n",
      "2023-03-07 22:06:13 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:06:32 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1012 steps:\n",
      "2023-03-07 22:06:32 - Cosine-Similarity :\tPearson: 0.8485\tSpearman: 0.8572\n",
      "2023-03-07 22:06:32 - Manhattan-Distance:\tPearson: 0.8391\tSpearman: 0.8376\n",
      "2023-03-07 22:06:32 - Euclidean-Distance:\tPearson: 0.8414\tSpearman: 0.8400\n",
      "2023-03-07 22:06:32 - Dot-Product-Similarity:\tPearson: 0.7789\tSpearman: 0.7802\n",
      "2023-03-07 22:06:50 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1056 steps:\n",
      "2023-03-07 22:06:50 - Cosine-Similarity :\tPearson: 0.8526\tSpearman: 0.8605\n",
      "2023-03-07 22:06:50 - Manhattan-Distance:\tPearson: 0.8432\tSpearman: 0.8421\n",
      "2023-03-07 22:06:50 - Euclidean-Distance:\tPearson: 0.8454\tSpearman: 0.8445\n",
      "2023-03-07 22:06:50 - Dot-Product-Similarity:\tPearson: 0.7919\tSpearman: 0.7930\n",
      "2023-03-07 22:06:50 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:07:09 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1100 steps:\n",
      "2023-03-07 22:07:09 - Cosine-Similarity :\tPearson: 0.8486\tSpearman: 0.8564\n",
      "2023-03-07 22:07:09 - Manhattan-Distance:\tPearson: 0.8439\tSpearman: 0.8425\n",
      "2023-03-07 22:07:09 - Euclidean-Distance:\tPearson: 0.8457\tSpearman: 0.8445\n",
      "2023-03-07 22:07:09 - Dot-Product-Similarity:\tPearson: 0.7799\tSpearman: 0.7787\n",
      "2023-03-07 22:07:26 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1144 steps:\n",
      "2023-03-07 22:07:27 - Cosine-Similarity :\tPearson: 0.8496\tSpearman: 0.8576\n",
      "2023-03-07 22:07:27 - Manhattan-Distance:\tPearson: 0.8443\tSpearman: 0.8430\n",
      "2023-03-07 22:07:27 - Euclidean-Distance:\tPearson: 0.8468\tSpearman: 0.8455\n",
      "2023-03-07 22:07:27 - Dot-Product-Similarity:\tPearson: 0.7847\tSpearman: 0.7838\n",
      "2023-03-07 22:07:44 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1188 steps:\n",
      "2023-03-07 22:07:45 - Cosine-Similarity :\tPearson: 0.8482\tSpearman: 0.8550\n",
      "2023-03-07 22:07:45 - Manhattan-Distance:\tPearson: 0.8409\tSpearman: 0.8401\n",
      "2023-03-07 22:07:45 - Euclidean-Distance:\tPearson: 0.8431\tSpearman: 0.8423\n",
      "2023-03-07 22:07:45 - Dot-Product-Similarity:\tPearson: 0.7912\tSpearman: 0.7888\n",
      "2023-03-07 22:08:02 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1232 steps:\n",
      "2023-03-07 22:08:03 - Cosine-Similarity :\tPearson: 0.8538\tSpearman: 0.8595\n",
      "2023-03-07 22:08:03 - Manhattan-Distance:\tPearson: 0.8435\tSpearman: 0.8423\n",
      "2023-03-07 22:08:03 - Euclidean-Distance:\tPearson: 0.8453\tSpearman: 0.8443\n",
      "2023-03-07 22:08:03 - Dot-Product-Similarity:\tPearson: 0.7958\tSpearman: 0.7948\n",
      "2023-03-07 22:08:19 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1276 steps:\n",
      "2023-03-07 22:08:20 - Cosine-Similarity :\tPearson: 0.8542\tSpearman: 0.8603\n",
      "2023-03-07 22:08:20 - Manhattan-Distance:\tPearson: 0.8456\tSpearman: 0.8453\n",
      "2023-03-07 22:08:20 - Euclidean-Distance:\tPearson: 0.8478\tSpearman: 0.8475\n",
      "2023-03-07 22:08:20 - Dot-Product-Similarity:\tPearson: 0.7993\tSpearman: 0.7980\n",
      "2023-03-07 22:08:37 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1320 steps:\n",
      "2023-03-07 22:08:38 - Cosine-Similarity :\tPearson: 0.8485\tSpearman: 0.8560\n",
      "2023-03-07 22:08:38 - Manhattan-Distance:\tPearson: 0.8394\tSpearman: 0.8392\n",
      "2023-03-07 22:08:38 - Euclidean-Distance:\tPearson: 0.8420\tSpearman: 0.8421\n",
      "2023-03-07 22:08:38 - Dot-Product-Similarity:\tPearson: 0.7787\tSpearman: 0.7800\n",
      "2023-03-07 22:08:55 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1364 steps:\n",
      "2023-03-07 22:08:56 - Cosine-Similarity :\tPearson: 0.8544\tSpearman: 0.8616\n",
      "2023-03-07 22:08:56 - Manhattan-Distance:\tPearson: 0.8419\tSpearman: 0.8416\n",
      "2023-03-07 22:08:56 - Euclidean-Distance:\tPearson: 0.8440\tSpearman: 0.8439\n",
      "2023-03-07 22:08:56 - Dot-Product-Similarity:\tPearson: 0.7883\tSpearman: 0.7903\n",
      "2023-03-07 22:08:56 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:09:15 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1408 steps:\n",
      "2023-03-07 22:09:16 - Cosine-Similarity :\tPearson: 0.8489\tSpearman: 0.8577\n",
      "2023-03-07 22:09:16 - Manhattan-Distance:\tPearson: 0.8420\tSpearman: 0.8417\n",
      "2023-03-07 22:09:16 - Euclidean-Distance:\tPearson: 0.8443\tSpearman: 0.8443\n",
      "2023-03-07 22:09:16 - Dot-Product-Similarity:\tPearson: 0.7937\tSpearman: 0.7937\n",
      "2023-03-07 22:09:33 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1452 steps:\n",
      "2023-03-07 22:09:33 - Cosine-Similarity :\tPearson: 0.8569\tSpearman: 0.8632\n",
      "2023-03-07 22:09:33 - Manhattan-Distance:\tPearson: 0.8419\tSpearman: 0.8415\n",
      "2023-03-07 22:09:33 - Euclidean-Distance:\tPearson: 0.8442\tSpearman: 0.8443\n",
      "2023-03-07 22:09:33 - Dot-Product-Similarity:\tPearson: 0.7974\tSpearman: 0.7985\n",
      "2023-03-07 22:09:33 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:09:52 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1496 steps:\n",
      "2023-03-07 22:09:53 - Cosine-Similarity :\tPearson: 0.8571\tSpearman: 0.8643\n",
      "2023-03-07 22:09:53 - Manhattan-Distance:\tPearson: 0.8489\tSpearman: 0.8488\n",
      "2023-03-07 22:09:53 - Euclidean-Distance:\tPearson: 0.8514\tSpearman: 0.8516\n",
      "2023-03-07 22:09:53 - Dot-Product-Similarity:\tPearson: 0.8014\tSpearman: 0.8015\n",
      "2023-03-07 22:09:53 - Save model to output/training_nli_v2_distilroberta-base-2023-03-07_21-59-05\n",
      "2023-03-07 22:10:12 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1540 steps:\n",
      "2023-03-07 22:10:13 - Cosine-Similarity :\tPearson: 0.8545\tSpearman: 0.8613\n",
      "2023-03-07 22:10:13 - Manhattan-Distance:\tPearson: 0.8478\tSpearman: 0.8474\n",
      "2023-03-07 22:10:13 - Euclidean-Distance:\tPearson: 0.8499\tSpearman: 0.8495\n",
      "2023-03-07 22:10:13 - Dot-Product-Similarity:\tPearson: 0.7959\tSpearman: 0.7951\n",
      "2023-03-07 22:10:30 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1584 steps:\n",
      "2023-03-07 22:10:30 - Cosine-Similarity :\tPearson: 0.8519\tSpearman: 0.8587\n",
      "2023-03-07 22:10:30 - Manhattan-Distance:\tPearson: 0.8446\tSpearman: 0.8440\n",
      "2023-03-07 22:10:30 - Euclidean-Distance:\tPearson: 0.8470\tSpearman: 0.8466\n",
      "2023-03-07 22:10:30 - Dot-Product-Similarity:\tPearson: 0.7932\tSpearman: 0.7925\n",
      "2023-03-07 22:10:48 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1628 steps:\n",
      "2023-03-07 22:10:48 - Cosine-Similarity :\tPearson: 0.8535\tSpearman: 0.8605\n",
      "2023-03-07 22:10:48 - Manhattan-Distance:\tPearson: 0.8435\tSpearman: 0.8430\n",
      "2023-03-07 22:10:48 - Euclidean-Distance:\tPearson: 0.8462\tSpearman: 0.8457\n",
      "2023-03-07 22:10:48 - Dot-Product-Similarity:\tPearson: 0.7990\tSpearman: 0.7988\n",
      "2023-03-07 22:11:05 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1672 steps:\n",
      "2023-03-07 22:11:06 - Cosine-Similarity :\tPearson: 0.8544\tSpearman: 0.8611\n",
      "2023-03-07 22:11:06 - Manhattan-Distance:\tPearson: 0.8446\tSpearman: 0.8437\n",
      "2023-03-07 22:11:06 - Euclidean-Distance:\tPearson: 0.8470\tSpearman: 0.8460\n",
      "2023-03-07 22:11:06 - Dot-Product-Similarity:\tPearson: 0.7895\tSpearman: 0.7885\n",
      "2023-03-07 22:11:23 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1716 steps:\n",
      "2023-03-07 22:11:23 - Cosine-Similarity :\tPearson: 0.8560\tSpearman: 0.8620\n",
      "2023-03-07 22:11:23 - Manhattan-Distance:\tPearson: 0.8513\tSpearman: 0.8508\n",
      "2023-03-07 22:11:23 - Euclidean-Distance:\tPearson: 0.8535\tSpearman: 0.8534\n",
      "2023-03-07 22:11:23 - Dot-Product-Similarity:\tPearson: 0.8094\tSpearman: 0.8073\n",
      "2023-03-07 22:11:41 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1760 steps:\n",
      "2023-03-07 22:11:41 - Cosine-Similarity :\tPearson: 0.8513\tSpearman: 0.8589\n",
      "2023-03-07 22:11:41 - Manhattan-Distance:\tPearson: 0.8440\tSpearman: 0.8431\n",
      "2023-03-07 22:11:41 - Euclidean-Distance:\tPearson: 0.8464\tSpearman: 0.8457\n",
      "2023-03-07 22:11:41 - Dot-Product-Similarity:\tPearson: 0.7913\tSpearman: 0.7904\n",
      "2023-03-07 22:11:58 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1804 steps:\n",
      "2023-03-07 22:11:59 - Cosine-Similarity :\tPearson: 0.8547\tSpearman: 0.8605\n",
      "2023-03-07 22:11:59 - Manhattan-Distance:\tPearson: 0.8434\tSpearman: 0.8426\n",
      "2023-03-07 22:11:59 - Euclidean-Distance:\tPearson: 0.8458\tSpearman: 0.8454\n",
      "2023-03-07 22:11:59 - Dot-Product-Similarity:\tPearson: 0.8010\tSpearman: 0.8009\n",
      "2023-03-07 22:12:16 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1848 steps:\n",
      "2023-03-07 22:12:17 - Cosine-Similarity :\tPearson: 0.8505\tSpearman: 0.8575\n",
      "2023-03-07 22:12:17 - Manhattan-Distance:\tPearson: 0.8437\tSpearman: 0.8427\n",
      "2023-03-07 22:12:17 - Euclidean-Distance:\tPearson: 0.8464\tSpearman: 0.8460\n",
      "2023-03-07 22:12:17 - Dot-Product-Similarity:\tPearson: 0.7983\tSpearman: 0.7974\n",
      "2023-03-07 22:12:34 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1892 steps:\n",
      "2023-03-07 22:12:34 - Cosine-Similarity :\tPearson: 0.8506\tSpearman: 0.8584\n",
      "2023-03-07 22:12:34 - Manhattan-Distance:\tPearson: 0.8446\tSpearman: 0.8440\n",
      "2023-03-07 22:12:34 - Euclidean-Distance:\tPearson: 0.8474\tSpearman: 0.8470\n",
      "2023-03-07 22:12:34 - Dot-Product-Similarity:\tPearson: 0.7924\tSpearman: 0.7919\n",
      "2023-03-07 22:12:52 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1936 steps:\n",
      "2023-03-07 22:12:53 - Cosine-Similarity :\tPearson: 0.8518\tSpearman: 0.8585\n",
      "2023-03-07 22:12:53 - Manhattan-Distance:\tPearson: 0.8428\tSpearman: 0.8428\n",
      "2023-03-07 22:12:53 - Euclidean-Distance:\tPearson: 0.8458\tSpearman: 0.8458\n",
      "2023-03-07 22:12:53 - Dot-Product-Similarity:\tPearson: 0.7886\tSpearman: 0.7880\n",
      "2023-03-07 22:13:10 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 1980 steps:\n",
      "2023-03-07 22:13:10 - Cosine-Similarity :\tPearson: 0.8551\tSpearman: 0.8618\n",
      "2023-03-07 22:13:10 - Manhattan-Distance:\tPearson: 0.8423\tSpearman: 0.8426\n",
      "2023-03-07 22:13:10 - Euclidean-Distance:\tPearson: 0.8451\tSpearman: 0.8455\n",
      "2023-03-07 22:13:10 - Dot-Product-Similarity:\tPearson: 0.7925\tSpearman: 0.7941\n",
      "2023-03-07 22:13:27 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2024 steps:\n",
      "2023-03-07 22:13:28 - Cosine-Similarity :\tPearson: 0.8534\tSpearman: 0.8603\n",
      "2023-03-07 22:13:28 - Manhattan-Distance:\tPearson: 0.8439\tSpearman: 0.8441\n",
      "2023-03-07 22:13:28 - Euclidean-Distance:\tPearson: 0.8466\tSpearman: 0.8469\n",
      "2023-03-07 22:13:28 - Dot-Product-Similarity:\tPearson: 0.7983\tSpearman: 0.7987\n",
      "2023-03-07 22:13:45 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2068 steps:\n",
      "2023-03-07 22:13:46 - Cosine-Similarity :\tPearson: 0.8507\tSpearman: 0.8573\n",
      "2023-03-07 22:13:46 - Manhattan-Distance:\tPearson: 0.8430\tSpearman: 0.8423\n",
      "2023-03-07 22:13:46 - Euclidean-Distance:\tPearson: 0.8455\tSpearman: 0.8450\n",
      "2023-03-07 22:13:46 - Dot-Product-Similarity:\tPearson: 0.7935\tSpearman: 0.7929\n",
      "2023-03-07 22:14:03 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2112 steps:\n",
      "2023-03-07 22:14:03 - Cosine-Similarity :\tPearson: 0.8571\tSpearman: 0.8633\n",
      "2023-03-07 22:14:03 - Manhattan-Distance:\tPearson: 0.8453\tSpearman: 0.8448\n",
      "2023-03-07 22:14:03 - Euclidean-Distance:\tPearson: 0.8477\tSpearman: 0.8474\n",
      "2023-03-07 22:14:03 - Dot-Product-Similarity:\tPearson: 0.8060\tSpearman: 0.8059\n",
      "2023-03-07 22:14:21 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2156 steps:\n",
      "2023-03-07 22:14:21 - Cosine-Similarity :\tPearson: 0.8559\tSpearman: 0.8622\n",
      "2023-03-07 22:14:21 - Manhattan-Distance:\tPearson: 0.8421\tSpearman: 0.8415\n",
      "2023-03-07 22:14:21 - Euclidean-Distance:\tPearson: 0.8447\tSpearman: 0.8446\n",
      "2023-03-07 22:14:21 - Dot-Product-Similarity:\tPearson: 0.7979\tSpearman: 0.7990\n",
      "2023-03-07 22:14:38 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2200 steps:\n",
      "2023-03-07 22:14:39 - Cosine-Similarity :\tPearson: 0.8533\tSpearman: 0.8603\n",
      "2023-03-07 22:14:39 - Manhattan-Distance:\tPearson: 0.8468\tSpearman: 0.8465\n",
      "2023-03-07 22:14:39 - Euclidean-Distance:\tPearson: 0.8496\tSpearman: 0.8493\n",
      "2023-03-07 22:14:39 - Dot-Product-Similarity:\tPearson: 0.7995\tSpearman: 0.7990\n",
      "2023-03-07 22:14:56 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2244 steps:\n",
      "2023-03-07 22:14:57 - Cosine-Similarity :\tPearson: 0.8484\tSpearman: 0.8554\n",
      "2023-03-07 22:14:57 - Manhattan-Distance:\tPearson: 0.8381\tSpearman: 0.8379\n",
      "2023-03-07 22:14:57 - Euclidean-Distance:\tPearson: 0.8408\tSpearman: 0.8408\n",
      "2023-03-07 22:14:57 - Dot-Product-Similarity:\tPearson: 0.7900\tSpearman: 0.7898\n",
      "2023-03-07 22:15:14 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2288 steps:\n",
      "2023-03-07 22:15:15 - Cosine-Similarity :\tPearson: 0.8558\tSpearman: 0.8629\n",
      "2023-03-07 22:15:15 - Manhattan-Distance:\tPearson: 0.8472\tSpearman: 0.8469\n",
      "2023-03-07 22:15:15 - Euclidean-Distance:\tPearson: 0.8496\tSpearman: 0.8495\n",
      "2023-03-07 22:15:15 - Dot-Product-Similarity:\tPearson: 0.8009\tSpearman: 0.8000\n",
      "2023-03-07 22:15:32 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2332 steps:\n",
      "2023-03-07 22:15:33 - Cosine-Similarity :\tPearson: 0.8550\tSpearman: 0.8611\n",
      "2023-03-07 22:15:33 - Manhattan-Distance:\tPearson: 0.8465\tSpearman: 0.8462\n",
      "2023-03-07 22:15:33 - Euclidean-Distance:\tPearson: 0.8490\tSpearman: 0.8488\n",
      "2023-03-07 22:15:33 - Dot-Product-Similarity:\tPearson: 0.8011\tSpearman: 0.8000\n",
      "2023-03-07 22:15:50 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2376 steps:\n",
      "2023-03-07 22:15:51 - Cosine-Similarity :\tPearson: 0.8574\tSpearman: 0.8628\n",
      "2023-03-07 22:15:51 - Manhattan-Distance:\tPearson: 0.8463\tSpearman: 0.8459\n",
      "2023-03-07 22:15:51 - Euclidean-Distance:\tPearson: 0.8487\tSpearman: 0.8485\n",
      "2023-03-07 22:15:51 - Dot-Product-Similarity:\tPearson: 0.8104\tSpearman: 0.8092\n",
      "2023-03-07 22:16:08 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2420 steps:\n",
      "2023-03-07 22:16:09 - Cosine-Similarity :\tPearson: 0.8542\tSpearman: 0.8606\n",
      "2023-03-07 22:16:09 - Manhattan-Distance:\tPearson: 0.8441\tSpearman: 0.8442\n",
      "2023-03-07 22:16:09 - Euclidean-Distance:\tPearson: 0.8467\tSpearman: 0.8468\n",
      "2023-03-07 22:16:09 - Dot-Product-Similarity:\tPearson: 0.8007\tSpearman: 0.7997\n",
      "2023-03-07 22:16:26 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2464 steps:\n",
      "2023-03-07 22:16:27 - Cosine-Similarity :\tPearson: 0.8543\tSpearman: 0.8604\n",
      "2023-03-07 22:16:27 - Manhattan-Distance:\tPearson: 0.8470\tSpearman: 0.8475\n",
      "2023-03-07 22:16:27 - Euclidean-Distance:\tPearson: 0.8493\tSpearman: 0.8499\n",
      "2023-03-07 22:16:27 - Dot-Product-Similarity:\tPearson: 0.8017\tSpearman: 0.8007\n",
      "2023-03-07 22:16:43 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2508 steps:\n",
      "2023-03-07 22:16:44 - Cosine-Similarity :\tPearson: 0.8526\tSpearman: 0.8587\n",
      "2023-03-07 22:16:44 - Manhattan-Distance:\tPearson: 0.8455\tSpearman: 0.8453\n",
      "2023-03-07 22:16:44 - Euclidean-Distance:\tPearson: 0.8480\tSpearman: 0.8479\n",
      "2023-03-07 22:16:44 - Dot-Product-Similarity:\tPearson: 0.7999\tSpearman: 0.7982\n",
      "2023-03-07 22:17:01 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2552 steps:\n",
      "2023-03-07 22:17:01 - Cosine-Similarity :\tPearson: 0.8536\tSpearman: 0.8591\n",
      "2023-03-07 22:17:01 - Manhattan-Distance:\tPearson: 0.8439\tSpearman: 0.8438\n",
      "2023-03-07 22:17:01 - Euclidean-Distance:\tPearson: 0.8463\tSpearman: 0.8465\n",
      "2023-03-07 22:17:01 - Dot-Product-Similarity:\tPearson: 0.8036\tSpearman: 0.8030\n",
      "2023-03-07 22:17:19 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2596 steps:\n",
      "2023-03-07 22:17:19 - Cosine-Similarity :\tPearson: 0.8549\tSpearman: 0.8607\n",
      "2023-03-07 22:17:19 - Manhattan-Distance:\tPearson: 0.8428\tSpearman: 0.8428\n",
      "2023-03-07 22:17:19 - Euclidean-Distance:\tPearson: 0.8452\tSpearman: 0.8455\n",
      "2023-03-07 22:17:19 - Dot-Product-Similarity:\tPearson: 0.7976\tSpearman: 0.7978\n",
      "2023-03-07 22:17:36 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2640 steps:\n",
      "2023-03-07 22:17:37 - Cosine-Similarity :\tPearson: 0.8540\tSpearman: 0.8607\n",
      "2023-03-07 22:17:37 - Manhattan-Distance:\tPearson: 0.8477\tSpearman: 0.8479\n",
      "2023-03-07 22:17:37 - Euclidean-Distance:\tPearson: 0.8499\tSpearman: 0.8500\n",
      "2023-03-07 22:17:37 - Dot-Product-Similarity:\tPearson: 0.8019\tSpearman: 0.8011\n",
      "2023-03-07 22:17:54 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2684 steps:\n",
      "2023-03-07 22:17:54 - Cosine-Similarity :\tPearson: 0.8559\tSpearman: 0.8612\n",
      "2023-03-07 22:17:54 - Manhattan-Distance:\tPearson: 0.8439\tSpearman: 0.8439\n",
      "2023-03-07 22:17:54 - Euclidean-Distance:\tPearson: 0.8462\tSpearman: 0.8464\n",
      "2023-03-07 22:17:54 - Dot-Product-Similarity:\tPearson: 0.8007\tSpearman: 0.8005\n",
      "2023-03-07 22:18:12 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2728 steps:\n",
      "2023-03-07 22:18:12 - Cosine-Similarity :\tPearson: 0.8530\tSpearman: 0.8587\n",
      "2023-03-07 22:18:12 - Manhattan-Distance:\tPearson: 0.8439\tSpearman: 0.8444\n",
      "2023-03-07 22:18:12 - Euclidean-Distance:\tPearson: 0.8465\tSpearman: 0.8472\n",
      "2023-03-07 22:18:12 - Dot-Product-Similarity:\tPearson: 0.7969\tSpearman: 0.7967\n",
      "2023-03-07 22:18:29 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2772 steps:\n",
      "2023-03-07 22:18:30 - Cosine-Similarity :\tPearson: 0.8557\tSpearman: 0.8615\n",
      "2023-03-07 22:18:30 - Manhattan-Distance:\tPearson: 0.8481\tSpearman: 0.8483\n",
      "2023-03-07 22:18:30 - Euclidean-Distance:\tPearson: 0.8504\tSpearman: 0.8510\n",
      "2023-03-07 22:18:30 - Dot-Product-Similarity:\tPearson: 0.7969\tSpearman: 0.7951\n",
      "2023-03-07 22:18:48 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2816 steps:\n",
      "2023-03-07 22:18:48 - Cosine-Similarity :\tPearson: 0.8555\tSpearman: 0.8608\n",
      "2023-03-07 22:18:48 - Manhattan-Distance:\tPearson: 0.8454\tSpearman: 0.8457\n",
      "2023-03-07 22:18:48 - Euclidean-Distance:\tPearson: 0.8478\tSpearman: 0.8484\n",
      "2023-03-07 22:18:48 - Dot-Product-Similarity:\tPearson: 0.7957\tSpearman: 0.7944\n",
      "2023-03-07 22:19:06 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2860 steps:\n",
      "2023-03-07 22:19:06 - Cosine-Similarity :\tPearson: 0.8545\tSpearman: 0.8603\n",
      "2023-03-07 22:19:06 - Manhattan-Distance:\tPearson: 0.8414\tSpearman: 0.8424\n",
      "2023-03-07 22:19:06 - Euclidean-Distance:\tPearson: 0.8439\tSpearman: 0.8448\n",
      "2023-03-07 22:19:06 - Dot-Product-Similarity:\tPearson: 0.7961\tSpearman: 0.7963\n",
      "2023-03-07 22:19:23 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2904 steps:\n",
      "2023-03-07 22:19:24 - Cosine-Similarity :\tPearson: 0.8553\tSpearman: 0.8615\n",
      "2023-03-07 22:19:24 - Manhattan-Distance:\tPearson: 0.8458\tSpearman: 0.8459\n",
      "2023-03-07 22:19:24 - Euclidean-Distance:\tPearson: 0.8481\tSpearman: 0.8484\n",
      "2023-03-07 22:19:24 - Dot-Product-Similarity:\tPearson: 0.7946\tSpearman: 0.7942\n",
      "2023-03-07 22:19:42 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2948 steps:\n",
      "2023-03-07 22:19:42 - Cosine-Similarity :\tPearson: 0.8543\tSpearman: 0.8598\n",
      "2023-03-07 22:19:42 - Manhattan-Distance:\tPearson: 0.8434\tSpearman: 0.8434\n",
      "2023-03-07 22:19:42 - Euclidean-Distance:\tPearson: 0.8459\tSpearman: 0.8460\n",
      "2023-03-07 22:19:42 - Dot-Product-Similarity:\tPearson: 0.7978\tSpearman: 0.7969\n",
      "2023-03-07 22:19:59 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 2992 steps:\n",
      "2023-03-07 22:20:00 - Cosine-Similarity :\tPearson: 0.8547\tSpearman: 0.8602\n",
      "2023-03-07 22:20:00 - Manhattan-Distance:\tPearson: 0.8409\tSpearman: 0.8407\n",
      "2023-03-07 22:20:00 - Euclidean-Distance:\tPearson: 0.8437\tSpearman: 0.8435\n",
      "2023-03-07 22:20:00 - Dot-Product-Similarity:\tPearson: 0.7933\tSpearman: 0.7934\n",
      "2023-03-07 22:20:17 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 3036 steps:\n",
      "2023-03-07 22:20:18 - Cosine-Similarity :\tPearson: 0.8540\tSpearman: 0.8596\n",
      "2023-03-07 22:20:18 - Manhattan-Distance:\tPearson: 0.8447\tSpearman: 0.8443\n",
      "2023-03-07 22:20:18 - Euclidean-Distance:\tPearson: 0.8472\tSpearman: 0.8471\n",
      "2023-03-07 22:20:18 - Dot-Product-Similarity:\tPearson: 0.8032\tSpearman: 0.8027\n",
      "2023-03-07 22:20:35 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 3080 steps:\n",
      "2023-03-07 22:20:36 - Cosine-Similarity :\tPearson: 0.8546\tSpearman: 0.8601\n",
      "2023-03-07 22:20:36 - Manhattan-Distance:\tPearson: 0.8450\tSpearman: 0.8444\n",
      "2023-03-07 22:20:36 - Euclidean-Distance:\tPearson: 0.8474\tSpearman: 0.8473\n",
      "2023-03-07 22:20:36 - Dot-Product-Similarity:\tPearson: 0.8062\tSpearman: 0.8045\n",
      "2023-03-07 22:20:53 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 3124 steps:\n",
      "2023-03-07 22:20:53 - Cosine-Similarity :\tPearson: 0.8554\tSpearman: 0.8606\n",
      "2023-03-07 22:20:53 - Manhattan-Distance:\tPearson: 0.8408\tSpearman: 0.8410\n",
      "2023-03-07 22:20:53 - Euclidean-Distance:\tPearson: 0.8434\tSpearman: 0.8437\n",
      "2023-03-07 22:20:53 - Dot-Product-Similarity:\tPearson: 0.7986\tSpearman: 0.7992\n",
      "2023-03-07 22:21:11 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 3168 steps:\n",
      "2023-03-07 22:21:11 - Cosine-Similarity :\tPearson: 0.8577\tSpearman: 0.8636\n",
      "2023-03-07 22:21:11 - Manhattan-Distance:\tPearson: 0.8471\tSpearman: 0.8472\n",
      "2023-03-07 22:21:11 - Euclidean-Distance:\tPearson: 0.8494\tSpearman: 0.8497\n",
      "2023-03-07 22:21:11 - Dot-Product-Similarity:\tPearson: 0.8039\tSpearman: 0.8031\n",
      "2023-03-07 22:21:29 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset in epoch 0 after 3212 steps:\n",
      "2023-03-07 22:21:29 - Cosine-Similarity :\tPearson: 0.8566\tSpearman: 0.8624\n",
      "2023-03-07 22:21:29 - Manhattan-Distance:\tPearson: 0.8452\tSpearman: 0.8455\n",
      "2023-03-07 22:21:29 - Euclidean-Distance:\tPearson: 0.8476\tSpearman: 0.8482\n",
      "2023-03-07 22:21:29 - Dot-Product-Similarity:\tPearson: 0.8052\tSpearman: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 # Train the model</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>113 model.fit(train_objectives=[(train_dataloader, train_loss)],                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 │   │     </span>evaluator=dev_evaluator,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 │   │     </span>epochs=num_epochs,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   │     </span>evaluation_steps=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(train_dataloader)*<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.01</span>),                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ubuntu/.local/lib/python3.10/site-packages/sentence_transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">SentenceTransformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">72</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fit</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">718 │   │   │   │   │   │   </span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">719 │   │   │   │   │   │   </span>skip_scheduler = scaler.get_scale() != scale_before_step           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">720 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>721 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>loss_value = loss_model(features, labels)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">722 │   │   │   │   │   │   </span>loss_value.backward()                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">723 │   │   │   │   │   │   </span>torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">724 │   │   │   │   │   │   </span>optimizer.step()                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ubuntu/.local/lib/python3.10/site-packages/sentence_transformers/losses/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">MultipleNegativesR</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ankingLoss.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">58</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55 │   │   </span>embeddings_b = torch.cat(reps[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:])                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">56 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">57 │   │   </span>scores = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.similarity_fct(embeddings_a, embeddings_b) * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scale               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>58 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>labels = torch.tensor(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(scores)), dtype=torch.long, device=scores.device    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">59 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cross_entropy_loss(scores, labels)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">60 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">61 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_config_dict</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m110 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m# Train the model\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m113 model.fit(train_objectives=[(train_dataloader, train_loss)],                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │     \u001b[0mevaluator=dev_evaluator,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   │     \u001b[0mepochs=num_epochs,                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │     \u001b[0mevaluation_steps=\u001b[96mint\u001b[0m(\u001b[96mlen\u001b[0m(train_dataloader)*\u001b[94m0.01\u001b[0m),                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ubuntu/.local/lib/python3.10/site-packages/sentence_transformers/\u001b[0m\u001b[1;33mSentenceTransformer.py\u001b[0m:\u001b[94m72\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m1\u001b[0m in \u001b[92mfit\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m718 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m719 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mskip_scheduler = scaler.get_scale() != scale_before_step           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m720 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m721 \u001b[2m│   │   │   │   │   │   \u001b[0mloss_value = loss_model(features, labels)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m722 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mloss_value.backward()                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m723 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtorch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m724 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0moptimizer.step()                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m in \u001b[92m_call_impl\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ubuntu/.local/lib/python3.10/site-packages/sentence_transformers/losses/\u001b[0m\u001b[1;33mMultipleNegativesR\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mankingLoss.py\u001b[0m:\u001b[94m58\u001b[0m in \u001b[92mforward\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m│   │   \u001b[0membeddings_b = torch.cat(reps[\u001b[94m1\u001b[0m:])                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m56 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m57 \u001b[0m\u001b[2m│   │   \u001b[0mscores = \u001b[96mself\u001b[0m.similarity_fct(embeddings_a, embeddings_b) * \u001b[96mself\u001b[0m.scale               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m58 \u001b[2m│   │   \u001b[0mlabels = torch.tensor(\u001b[96mrange\u001b[0m(\u001b[96mlen\u001b[0m(scores)), dtype=torch.long, device=scores.device    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m59 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.cross_entropy_loss(scores, labels)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m60 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m61 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mget_config_dict\u001b[0m(\u001b[96mself\u001b[0m):                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The system trains BERT (or any other transformer model like RoBERTa, DistilBERT etc.) on the SNLI + MultiNLI (AllNLI) dataset\n",
    "with MultipleNegativesRankingLoss. Entailnments are poisitive pairs and the contradiction on AllNLI dataset is added as a hard negative.\n",
    "At every 10% training steps, the model is evaluated on the STS benchmark dataset\n",
    "Usage:\n",
    "python training_nli_v2.py\n",
    "OR\n",
    "python training_nli_v2.py pretrained_transformer_model_name\n",
    "\"\"\"\n",
    "import math\n",
    "from sentence_transformers import models, losses, datasets\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import random\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "model_name = 'distilroberta-base'\n",
    "train_batch_size = 128          #The larger you select this, the better the results (usually). But it requires more GPU memory\n",
    "max_seq_length = 75\n",
    "num_epochs = 1\n",
    "\n",
    "# Save path of the model\n",
    "model_save_path = 'output/training_nli_v2_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here we define our SentenceTransformer model\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='mean')\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if dataset exsist. If not, download and extract  it\n",
    "nli_dataset_path = 'data/AllNLI.tsv.gz'\n",
    "sts_dataset_path = 'data/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the AllNLI.tsv.gz file and create the training dataset\n",
    "logging.info(\"Read AllNLI train dataset\")\n",
    "\n",
    "def add_to_samples(sent1, sent2, label):\n",
    "    if sent1 not in train_data:\n",
    "        train_data[sent1] = {'contradiction': set(), 'entailment': set(), 'neutral': set()}\n",
    "    train_data[sent1][label].add(sent2)\n",
    "\n",
    "\n",
    "train_data = {}\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'train':\n",
    "            sent1 = row['sentence1'].strip()\n",
    "            sent2 = row['sentence2'].strip()\n",
    "\n",
    "            add_to_samples(sent1, sent2, row['label'])\n",
    "            add_to_samples(sent2, sent1, row['label'])  #Also add the opposite\n",
    "\n",
    "\n",
    "train_samples = []\n",
    "for sent1, others in train_data.items():\n",
    "    if len(others['entailment']) > 0 and len(others['contradiction']) > 0:\n",
    "        train_samples.append(InputExample(texts=[sent1, random.choice(list(others['entailment'])), random.choice(list(others['contradiction']))]))\n",
    "        train_samples.append(InputExample(texts=[random.choice(list(others['entailment'])), sent1, random.choice(list(others['contradiction']))]))\n",
    "\n",
    "logging.info(\"Train samples: {}\".format(len(train_samples)))\n",
    "\n",
    "# Special data loader that avoid duplicates within a batch\n",
    "train_dataloader = datasets.NoDuplicatesDataLoader(train_samples, batch_size=train_batch_size)\n",
    "\n",
    "\n",
    "# Our training loss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Read STSbenchmark dataset and use it as development set\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "dev_samples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'dev':\n",
    "            score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
    "            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, batch_size=train_batch_size, name='sts-dev')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure the training\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=dev_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=int(len(train_dataloader)*0.01),\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path,\n",
    "          use_amp=False          #Set to True, if your GPU supports FP16 operations\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "test_samples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'test':\n",
    "            score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
    "            test_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, batch_size=train_batch_size, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
